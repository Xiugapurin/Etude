{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa20ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from corpus import Synchronizer\n",
    "\n",
    "def calculate_wps(origin_path: str, cover_path: str, song_dir: str, lambda_wps: float = 1.0) -> float:\n",
    "    try:\n",
    "        s = Synchronizer()\n",
    "        wp = s.get_wp(origin_path, cover_path, song_dir)\n",
    "        \n",
    "        t_cover = s.t1\n",
    "        t_orig = s.t2\n",
    "        \n",
    "        if t_cover is None or t_orig is None:\n",
    "            raise ValueError(\"æ™‚é–“æˆ³åºåˆ—æœªèƒ½æˆåŠŸåœ¨ Synchronizer ç‰©ä»¶ä¸­ç”Ÿæˆã€‚\")\n",
    "\n",
    "        wp_int = wp.astype(int)\n",
    "        path_t_cover = t_cover[wp_int[0]]\n",
    "        path_t_orig = t_orig[wp_int[1]]\n",
    "\n",
    "        coeffs = np.polyfit(path_t_cover, path_t_orig, 1)\n",
    "        a, b = coeffs[0], coeffs[1]\n",
    "\n",
    "        t_orig_predicted = a * path_t_cover + b\n",
    "        deviation = path_t_orig - t_orig_predicted\n",
    "        sigma_dev = np.std(deviation)\n",
    "        wps_score = np.exp(-lambda_wps * sigma_dev)\n",
    "        return wps_score\n",
    "    except Exception as e:\n",
    "        print(f\"è¨ˆç®— wps åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def analyze_and_visualize_scores():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½å¼ï¼šè¨ˆç®—æ‰€æœ‰æ­Œæ›²å„ç‰ˆæœ¬çš„ wps åˆ†æ•¸ï¼Œè¨ˆç®—å¹³å‡å€¼ï¼Œä¸¦é€²è¡Œè¦–è¦ºåŒ–ã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    base_dir = os.path.join(\".\", \"dataset\", \"eval\")\n",
    "    metadata_path = os.path.join(base_dir, \"metadata.json\")\n",
    "    origin_filename = \"origin.wav\"\n",
    "    versions = [\"human\", \"etude_e\", \"etude_d\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    lambda_val = 0.5\n",
    "    \n",
    "    # ç”¨æ–¼å„²å­˜æ‰€æœ‰åˆ†æ•¸çš„å­—å…¸\n",
    "    scores_by_version = {v: [] for v in versions}\n",
    "\n",
    "    # --- 2. è®€å– metadata ä¸¦è¨ˆç®—åˆ†æ•¸ ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        # print(f\"âœ… æˆåŠŸè®€å– metadata.jsonï¼Œå…±æ‰¾åˆ° {len(metadata)} é¦–æ­Œæ›²ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° metadata.json æª”æ¡ˆã€‚\")\n",
    "        return\n",
    "\n",
    "    for i, song_data in enumerate(metadata):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name:\n",
    "            continue\n",
    "\n",
    "        song_dir = os.path.join(base_dir, dir_name)\n",
    "        # print(f\"\\nğŸµ æ­£åœ¨è™•ç†æ­Œæ›²: {song_dir} ({i+1}/{len(metadata)})\")\n",
    "\n",
    "        origin_path = os.path.join(song_dir, origin_filename)\n",
    "        if not os.path.exists(origin_path):\n",
    "            print(f\"  â†ªï¸ å·²è·³é (æ‰¾ä¸åˆ° origin.wav), {origin_path}\")\n",
    "            continue\n",
    "\n",
    "        for v in versions:\n",
    "            cover_path = os.path.join(song_dir, f\"{v}.wav\")\n",
    "            if not os.path.exists(cover_path):\n",
    "                print(f\"  â†ªï¸ å·²è·³éç‰ˆæœ¬ '{v}' (æ‰¾ä¸åˆ° {v}.wav) {song_dir}\")\n",
    "                continue\n",
    "            \n",
    "            # è¨ˆç®—åˆ†æ•¸\n",
    "            score = calculate_wps(origin_path, cover_path, song_dir, lambda_wps=lambda_val)\n",
    "            if score > 0:\n",
    "                # print(f\"  ğŸ“Š ç‰ˆæœ¬ '{v}' çš„ wps åˆ†æ•¸: {score:.4f}\")\n",
    "                scores_by_version[v].append(score)\n",
    "\n",
    "    # --- 3. è¨ˆç®—ä¸¦æ‰“å°å¹³å‡åˆ†æ•¸ ---\n",
    "    print(\"\\n\\n--- å¹³å‡åˆ†æ•¸çµ±è¨ˆ ---\")\n",
    "    average_scores = {}\n",
    "    for version, scores in scores_by_version.items():\n",
    "        if scores:\n",
    "            avg_score = np.mean(scores)\n",
    "            average_scores[version] = avg_score\n",
    "            print(f\"ç‰ˆæœ¬ {version:<12}: å¹³å‡ wps åˆ†æ•¸ = {avg_score:.4f} (åŸºæ–¼ {len(scores)} å€‹æ¨£æœ¬)\")\n",
    "        else:\n",
    "            print(f\"ç‰ˆæœ¬ {version:<12}: ç„¡æœ‰æ•ˆåˆ†æ•¸å¯ä¾›è¨ˆç®—ã€‚\")\n",
    "    \n",
    "    # --- 4. æ•¸æ“šè¦–è¦ºåŒ– ---\n",
    "    print(\"\\nğŸ¨ æ­£åœ¨ç”Ÿæˆåˆ†æ•¸åˆ†ä½ˆåœ–...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # ç‚ºæ¯å€‹ç‰ˆæœ¬ç¹ªè£½ä¸€æ¢ KDE æ›²ç·š\n",
    "    for version, scores in scores_by_version.items():\n",
    "        if scores:\n",
    "            sns.kdeplot(scores, label=version, fill=True, alpha=0.5, ax=ax, lw=2.5)\n",
    "\n",
    "    ax.set_title('WPS Score Distribution by Version', fontsize=16, pad=20)\n",
    "    ax.set_xlabel('WPS Score', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_xlim(0, 1.05)\n",
    "    ax.legend(title='Version', fontsize=10)\n",
    "    \n",
    "    # å„²å­˜åœ–è¡¨\n",
    "    output_image_path = \"wps_score_distribution.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"âœ… åˆ†æ•¸åˆ†ä½ˆåœ–å·²æˆåŠŸå„²å­˜è‡³: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_and_visualize_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b11cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å‡è¨­æ‚¨çš„ Synchronizer ä½æ–¼æ­¤è™•\n",
    "from corpus import Synchronizer \n",
    "\n",
    "# ã€å‡½å¼å·²åŠ å…¥ trim_seconds åƒæ•¸ã€‘\n",
    "def calculate_nwpd(origin_path: str, cover_path: str, song_dir: str, \n",
    "                   lambda_nwpd: float = 1.0, subsample_step: int = 1, \n",
    "                   trim_seconds: float = 0) -> float:\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ­£è¦åŒ–è·¯å¾‘åå·® (NWPD) åˆ†æ•¸ã€‚\n",
    "\n",
    "    Args:\n",
    "        ... (åŸæœ‰åƒæ•¸) ...\n",
    "        subsample_step (int): å°é½Šè·¯å¾‘çš„é™æ¡æ¨£æ­¥é•·ã€‚\n",
    "        trim_seconds (float): å¾è·¯å¾‘çš„é ­å°¾å„è£å»å¹¾ç§’ä¸åƒèˆ‡è¨ˆç®—ã€‚è¨­ç‚º 0 è¡¨ç¤ºä¸è£åˆ‡ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(subsample_step, int) or subsample_step < 1:\n",
    "            raise ValueError(\"subsample_step å¿…é ˆæ˜¯ >= 1 çš„æ•´æ•¸ã€‚\")\n",
    "        if not isinstance(trim_seconds, (int, float)) or trim_seconds < 0:\n",
    "            raise ValueError(\"trim_seconds å¿…é ˆæ˜¯ >= 0 çš„æ•¸å­—ã€‚\")\n",
    "\n",
    "        s = Synchronizer()\n",
    "        wp = s.get_wp(origin_path, cover_path, song_dir)\n",
    "        \n",
    "        t_cover = s.t1\n",
    "        t_orig = s.t2\n",
    "        \n",
    "        if t_cover is None or t_orig is None:\n",
    "            raise ValueError(\"æ™‚é–“æˆ³åºåˆ—æœªèƒ½æˆåŠŸåœ¨ Synchronizer ç‰©ä»¶ä¸­ç”Ÿæˆã€‚\")\n",
    "\n",
    "        wp_int = wp.astype(int)\n",
    "        \n",
    "        # é™æ¡æ¨£\n",
    "        wp_to_process = wp_int[:, ::subsample_step] if subsample_step > 1 else wp_int\n",
    "        \n",
    "        if wp_to_process.shape[1] < 10: # é»å¤ªå°‘å‰‡ä¸é€²è¡Œå¾ŒçºŒè™•ç†\n",
    "            return 0.0\n",
    "\n",
    "        path_t_cover = t_cover[wp_to_process[0]]\n",
    "        path_t_orig = t_orig[wp_to_process[1]]\n",
    "\n",
    "        # --- ã€é—œéµä¿®æ”¹ #1ã€‘æ ¹æ“šç§’æ•¸è£åˆ‡å°é½Šè·¯å¾‘ ---\n",
    "        if trim_seconds > 0:\n",
    "            total_duration = path_t_orig[-1]\n",
    "            # ç¢ºä¿æ¨‚æ›²é•·åº¦è¶³å¤ é€²è¡Œè£åˆ‡\n",
    "            if total_duration > (2 * trim_seconds):\n",
    "                start_time = trim_seconds\n",
    "                end_time = total_duration - trim_seconds\n",
    "                \n",
    "                mask = (path_t_orig >= start_time) & (path_t_orig <= end_time)\n",
    "                \n",
    "                # æ‡‰ç”¨é®ç½©ï¼Œä¸¦ç¢ºä¿è£åˆ‡å¾Œä»æœ‰è¶³å¤ çš„é»\n",
    "                if np.sum(mask) > 10:\n",
    "                    path_t_cover = path_t_cover[mask]\n",
    "                    path_t_orig = path_t_orig[mask]\n",
    "        # ---\n",
    "\n",
    "        coeffs = np.polyfit(path_t_cover, path_t_orig, 1)\n",
    "        a, b = coeffs[0], coeffs[1]\n",
    "\n",
    "        t_orig_predicted = a * path_t_cover + b\n",
    "        deviation = path_t_orig - t_orig_predicted\n",
    "        sigma_dev = np.std(deviation)\n",
    "        nwpd_score = np.exp(-lambda_nwpd * sigma_dev)\n",
    "        return nwpd_score\n",
    "    except Exception as e:\n",
    "        print(f\"è¨ˆç®— NWPD åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def analyze_and_visualize_scores():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½å¼ï¼šè¨ˆç®—æ‰€æœ‰æ­Œæ›²å„ç‰ˆæœ¬çš„ NWPD åˆ†æ•¸ï¼Œè¨ˆç®—å¹³å‡å€¼ï¼Œä¸¦é€²è¡Œè¦–è¦ºåŒ–ã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    base_dir = os.path.join(\".\", \"dataset\", \"eval\")\n",
    "    metadata_path = os.path.join(base_dir, \"metadata.json\")\n",
    "    origin_filename = \"origin.wav\"\n",
    "    versions = [\"human\", \"etude_e\", \"etude_d\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    lambda_val = 0.5\n",
    "    \n",
    "    # ã€æ–°å¢ã€‘è¨­å®šè¦è£åˆ‡çš„é ­å°¾ç§’æ•¸ï¼Œè¨­ç‚º 0 è¡¨ç¤ºä¸è£åˆ‡\n",
    "    TRIM_SECONDS = 10\n",
    "    \n",
    "    # è¨­å®šé™æ¡æ¨£æ­¥é•·ï¼Œ1 è¡¨ç¤ºä¸é€²è¡Œé™æ¡æ¨£\n",
    "    SUBSAMPLE_STEP = 10 \n",
    "    \n",
    "    # è¨­å®šç”¨æ–¼åœ–è¡¨å’Œå ±å‘Šçš„é¡¯ç¤ºåç¨±\n",
    "    VERSION_DISPLAY_NAMES = {\n",
    "        \"human\": \"Human\",\n",
    "        \"etude_e\": \"Etude Extractor\",\n",
    "        \"etude_d\": \"Etude Decoder\",\n",
    "        \"picogen\": \"PiCoGen\",\n",
    "        \"amtapc\": \"AMT-APC\",\n",
    "        \"music2midi\": \"Music2MIDI\"\n",
    "    }\n",
    "    \n",
    "    results_list = []\n",
    "\n",
    "    # --- 2. è®€å– metadata ä¸¦è¨ˆç®—åˆ†æ•¸ ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"âœ… æˆåŠŸè®€å– metadata.jsonï¼Œå°‡åˆ†æ {len(metadata)} é¦–æ­Œæ›²ã€‚\")\n",
    "        print(f\"â„¹ï¸  Warping Path è£åˆ‡ç§’æ•¸è¨­å®šç‚º: {TRIM_SECONDS}s\")\n",
    "        print(f\"â„¹ï¸  Warping Path é™æ¡æ¨£æ­¥é•·è¨­å®šç‚º: {SUBSAMPLE_STEP}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° metadata.json æª”æ¡ˆã€‚\")\n",
    "        return\n",
    "\n",
    "    for i, song_data in enumerate(tqdm(metadata, desc=\"Analyzing Songs\")):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "\n",
    "        song_dir = os.path.join(base_dir, dir_name)\n",
    "        origin_path = os.path.join(song_dir, origin_filename)\n",
    "        if not os.path.exists(origin_path): continue\n",
    "\n",
    "        for v in versions:\n",
    "            cover_path = os.path.join(song_dir, f\"{v}.wav\")\n",
    "            if not os.path.exists(cover_path): continue\n",
    "            \n",
    "            # ã€é—œéµä¿®æ”¹ #2ã€‘å‘¼å«å‡½å¼æ™‚å‚³å…¥è£åˆ‡ç§’æ•¸\n",
    "            score = calculate_nwpd(origin_path, cover_path, song_dir, \n",
    "                                   lambda_nwpd=lambda_val, \n",
    "                                   subsample_step=SUBSAMPLE_STEP,\n",
    "                                   trim_seconds=TRIM_SECONDS)\n",
    "            if score > 0:\n",
    "                results_list.append({\n",
    "                    'Version': VERSION_DISPLAY_NAMES.get(v, v), # ç›´æ¥ä½¿ç”¨é¡¯ç¤ºåç¨±\n",
    "                    'WPS Score': score\n",
    "                })\n",
    "\n",
    "    # --- 3. è¨ˆç®—ä¸¦æ‰“å°å¹³å‡åˆ†æ•¸ ---\n",
    "    df = pd.DataFrame(results_list)\n",
    "    if df.empty:\n",
    "        print(\"æœªèƒ½è¨ˆç®—å‡ºä»»ä½•æœ‰æ•ˆåˆ†æ•¸ã€‚\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\n\\n--- å¹³å‡åˆ†æ•¸çµ±è¨ˆ ---\")\n",
    "    # æ ¹æ“šå¹³å‡åˆ†æ•¸ç”±é«˜åˆ°ä½æ’åº\n",
    "    sorted_means = df.groupby('Version')['WPS Score'].mean().sort_values(ascending=False)\n",
    "    print(sorted_means)\n",
    "    \n",
    "    # --- 4. æ•¸æ“šè¦–è¦ºåŒ– ---\n",
    "    print(\"\\nğŸ¨ æ­£åœ¨ç”Ÿæˆåˆ†æ•¸åˆ†ä½ˆåœ–...\")\n",
    "    \n",
    "    # ç²å–æ’åºå¾Œçš„é¡¯ç¤ºåç¨±åˆ—è¡¨\n",
    "    order = sorted_means.index\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    sns.boxplot(data=df, x='WPS Score', y='Version', order=order, palette='viridis', ax=ax)\n",
    "\n",
    "    title = f'WPS Distribution (Trim: {TRIM_SECONDS}s, Subsample: {SUBSAMPLE_STEP})'\n",
    "    ax.set_title(title, fontsize=16, pad=20)\n",
    "    ax.set_xlabel('WPS', fontsize=12)\n",
    "    ax.set_ylabel('Version', fontsize=12)\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # å„²å­˜åœ–è¡¨\n",
    "    output_image_path = f\"nwpd_score_dist_trim{int(TRIM_SECONDS)}_step{SUBSAMPLE_STEP}.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"âœ… åˆ†æ•¸åˆ†ä½ˆåœ–å·²æˆåŠŸå„²å­˜è‡³: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_and_visualize_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å‡è¨­æ‚¨çš„ Synchronizer ä½æ–¼æ­¤è™•\n",
    "from corpus import Synchronizer \n",
    "\n",
    "def calculate_nwpd_trimmed(origin_path: str, cover_path: str, song_dir: str, \n",
    "                           lambda_nwpd: float = 1.0, trim_seconds: float = 10.0) -> float:\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ­£è¦åŒ–è·¯å¾‘åå·® (NWPD) åˆ†æ•¸ã€‚\n",
    "    ã€æ–°ç‰ˆã€‘ï¼šæ­¤ç‰ˆæœ¬æœƒè£å»å°é½Šè·¯å¾‘çš„é ­å°¾éƒ¨åˆ†å†é€²è¡Œè¨ˆç®—ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = Synchronizer()\n",
    "        wp = s.get_wp(origin_path, cover_path, song_dir)\n",
    "        \n",
    "        t_cover = s.t1\n",
    "        t_orig = s.t2\n",
    "        \n",
    "        if t_cover is None or t_orig is None:\n",
    "            raise ValueError(\"æ™‚é–“æˆ³åºåˆ—æœªèƒ½æˆåŠŸåœ¨ Synchronizer ç‰©ä»¶ä¸­ç”Ÿæˆã€‚\")\n",
    "\n",
    "        wp_int = wp.astype(int)\n",
    "        path_t_cover = t_cover[wp_int[0]]\n",
    "        path_t_orig = t_orig[wp_int[1]]\n",
    "        \n",
    "        # --- ã€é—œéµä¿®æ”¹ã€‘è£åˆ‡å°é½Šè·¯å¾‘çš„é ­å°¾ ---\n",
    "        \n",
    "        # 1. æª¢æŸ¥æ¨‚æ›²ç¸½æ™‚é•·æ˜¯å¦è¶³å¤ é€²è¡Œè£åˆ‡\n",
    "        total_duration = path_t_orig[-1]\n",
    "        \n",
    "        trimmed_path_t_cover = path_t_cover\n",
    "        trimmed_path_t_orig = path_t_orig\n",
    "\n",
    "        if total_duration > (2 * trim_seconds):\n",
    "            # 2. æ‰¾å‡ºéœ€è¦ä¿ç•™çš„æ™‚é–“å€é–“\n",
    "            start_time = trim_seconds\n",
    "            end_time = total_duration - trim_seconds\n",
    "            \n",
    "            # 3. å»ºç«‹å¸ƒæ—é®ç½© (mask) ä¾†éæ¿¾åœ¨é€™å€‹å€é–“å…§çš„é»\n",
    "            mask = (path_t_orig >= start_time) & (path_t_orig <= end_time)\n",
    "            \n",
    "            # æ‡‰ç”¨é®ç½©\n",
    "            trimmed_path_t_cover = path_t_cover[mask]\n",
    "            trimmed_path_t_orig = path_t_orig[mask]\n",
    "            \n",
    "            # å¦‚æœè£åˆ‡å¾Œé»å¤ªå°‘ï¼Œå‰‡ä¸é€²è¡Œè£åˆ‡\n",
    "            if len(trimmed_path_t_orig) < 10:\n",
    "                trimmed_path_t_cover = path_t_cover\n",
    "                trimmed_path_t_orig = path_t_orig\n",
    "        \n",
    "        # --- å¾ŒçºŒè¨ˆç®—ä½¿ç”¨è£åˆ‡å¾Œçš„è·¯å¾‘ ---\n",
    "\n",
    "        coeffs = np.polyfit(trimmed_path_t_cover, trimmed_path_t_orig, 1)\n",
    "        a, b = coeffs[0], coeffs[1]\n",
    "\n",
    "        t_orig_predicted = a * trimmed_path_t_cover + b\n",
    "        deviation = trimmed_path_t_orig - t_orig_predicted\n",
    "        sigma_dev = np.std(deviation)\n",
    "        nwpd_score = np.exp(-lambda_nwpd * sigma_dev)\n",
    "        \n",
    "        return nwpd_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"è¨ˆç®— NWPD åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}, æª”æ¡ˆ: {os.path.basename(cover_path)}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def analyze_and_visualize_scores_detailed():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½å¼ï¼šè¨ˆç®—æ‰€æœ‰æ­Œæ›²å„ç‰ˆæœ¬çš„ NWPD åˆ†æ•¸ï¼ˆä½¿ç”¨è£åˆ‡ç‰ˆï¼‰ï¼Œä¸¦é€²è¡Œåˆ†æã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    base_dir = os.path.join(\".\", \"dataset\", \"eval\")\n",
    "    metadata_path = os.path.join(base_dir, \"metadata.json\")\n",
    "    origin_filename = \"origin.wav\"\n",
    "    versions = [\"human\", \"etude_e\", \"etude_d\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    lambda_val = 0.5\n",
    "    trim_seconds = 20.0 # è¨­å®šè¦è£åˆ‡çš„ç§’æ•¸\n",
    "    \n",
    "    results_list = []\n",
    "\n",
    "    # --- 2. è®€å– metadata ä¸¦è¨ˆç®—åˆ†æ•¸ ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"âœ… æˆåŠŸè®€å– metadata.jsonï¼Œå°‡åˆ†æ {len(metadata)} é¦–æ­Œæ›²ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° metadata.json æª”æ¡ˆã€‚\")\n",
    "        return\n",
    "\n",
    "    for song_data in tqdm(metadata, desc=\"Analyzing Songs\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "\n",
    "        song_dir = os.path.join(base_dir, dir_name)\n",
    "        origin_path = os.path.join(song_dir, origin_filename)\n",
    "        if not os.path.exists(origin_path): continue\n",
    "\n",
    "        for v in versions:\n",
    "            cover_path = os.path.join(song_dir, f\"{v}.wav\")\n",
    "            if not os.path.exists(cover_path): continue\n",
    "            \n",
    "            # ã€é—œéµä¿®æ”¹ã€‘å‘¼å«æ–°çš„è£åˆ‡ç‰ˆå‡½å¼\n",
    "            score = calculate_nwpd_trimmed(origin_path, cover_path, song_dir, \n",
    "                                           lambda_nwpd=lambda_val, trim_seconds=trim_seconds)\n",
    "            if score > 0:\n",
    "                results_list.append({\n",
    "                    'song': dir_name,\n",
    "                    'version': v,\n",
    "                    'nwpd_score': score\n",
    "                })\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"æœªèƒ½è¨ˆç®—å‡ºä»»ä½•æœ‰æ•ˆåˆ†æ•¸ã€‚\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # --- 3. è¼¸å‡ºè©³ç´°çµ±è¨ˆèˆ‡ä½åˆ†æ­Œæ›² ---\n",
    "    print(\"\\n\\n--- å„ç‰ˆæœ¬ã€è£åˆ‡å¾Œã€‘NWPD åˆ†æ•¸è©³ç´°çµ±è¨ˆ ---\")\n",
    "    detailed_stats = df.groupby('version')['nwpd_score'].describe().sort_values('mean', ascending=False)\n",
    "    print(detailed_stats)\n",
    "\n",
    "    print(\"\\n--- ã€è£åˆ‡å¾Œã€‘åˆ†æ•¸æœ€ä½çš„ 10 é¦– 'human' æ¼”å¥æ­Œæ›² ---\")\n",
    "    human_scores_df = df[df['version'] == 'human']\n",
    "    lowest_human_scores = human_scores_df.sort_values(by='nwpd_score').head(10)\n",
    "    print(lowest_human_scores)\n",
    "\n",
    "    # --- 4. æ•¸æ“šè¦–è¦ºåŒ– ---\n",
    "    print(\"\\nğŸ¨ æ­£åœ¨ç”Ÿæˆã€è£åˆ‡å¾Œã€‘åˆ†æ•¸åˆ†ä½ˆåœ– (Box Plot)...\")\n",
    "    order = detailed_stats.index\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(data=df, x='nwpd_score', y='version', order=order, palette='viridis')\n",
    "    plt.title(f'NWPD Score Distribution (Trimmed by {trim_seconds}s)', fontsize=18, pad=20)\n",
    "    plt.xlabel('NWPD Score (Higher is Better)', fontsize=14)\n",
    "    plt.ylabel('Version', fontsize=14)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    output_image_path = \"nwpd_score_boxplot_trimmed.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"âœ… åˆ†æ•¸åˆ†ä½ˆåœ–å·²æˆåŠŸå„²å­˜è‡³: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ç¢ºä¿ pandas, seaborn, tqdm å·²å®‰è£\n",
    "    # pip install pandas seaborn tqdm\n",
    "    analyze_and_visualize_scores_detailed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm # ç”¨æ–¼é¡¯ç¤ºé€²åº¦æ¢\n",
    "\n",
    "from evaluation import IPECalculator\n",
    "\n",
    "def analyze_dataset_for_ipe_params():\n",
    "    \"\"\"\n",
    "    éæ­·è³‡æ–™é›†ï¼Œè¨ˆç®—æ‰€æœ‰äººé¡æ¼”å¥çš„é¦™è¾²ç†µï¼Œä¸¦è¼¸å‡ºçµ±è¨ˆæ•¸æ“šä»¥æ±ºå®š IPE åƒæ•¸ã€‚\n",
    "    \"\"\"\n",
    "    dataset_dir = \"./dataset/synced/\"\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™é›†ç›®éŒ„ {dataset_dir}\")\n",
    "        return\n",
    "\n",
    "    # åˆå§‹åŒ–è¨ˆç®—å™¨ã€‚mu å’Œ sigma åœ¨é€™è£¡ä¸é‡è¦ï¼Œä½† n_gram å’Œ n_clusters æœƒå½±éŸ¿ç†µçš„è¨ˆç®—\n",
    "    ipe_calculator = IPECalculator(n_gram=5, n_clusters=16)\n",
    "    \n",
    "    entropy_values = []\n",
    "    \n",
    "    # ç²å–æ‰€æœ‰å­ç›®éŒ„\n",
    "    subdirectories = [d for d in os.scandir(dataset_dir) if d.is_dir()]\n",
    "    \n",
    "    print(f\"æ­£åœ¨åˆ†æ {len(subdirectories)} é¦–äººé¡æ¼”å¥æ­Œæ›²...\")\n",
    "    \n",
    "    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢\n",
    "    for entry in tqdm(subdirectories, desc=\"Analyzing songs\"):\n",
    "        json_path = os.path.join(entry.path, \"cover.json\")\n",
    "        \n",
    "        if os.path.exists(json_path):\n",
    "            # æˆ‘å€‘åªéœ€è¦è¨ˆç®—ç†µå€¼\n",
    "            results = ipe_calculator.calculate_ipe(json_path)\n",
    "            if \"shannon_entropy\" in results:\n",
    "                entropy_values.append(results[\"shannon_entropy\"])\n",
    "\n",
    "    if not entropy_values:\n",
    "        print(\"æœªèƒ½åœ¨è³‡æ–™é›†ä¸­è¨ˆç®—å‡ºä»»ä½•ç†µå€¼ã€‚\")\n",
    "        return\n",
    "        \n",
    "    # --- çµ±è¨ˆåˆ†æ ---\n",
    "    entropy_series = pd.Series(entropy_values)\n",
    "    stats = entropy_series.describe()\n",
    "    \n",
    "    mean_entropy = stats['mean']\n",
    "    std_entropy = stats['std']\n",
    "    \n",
    "    print(\"\\n\\n--- äººé¡æ¼”å¥è³‡æ–™é›†ç†µå€¼çµ±è¨ˆåˆ†æ ---\")\n",
    "    print(stats)\n",
    "    \n",
    "    print(\"\\n--- å»ºè­°çš„ IPE åƒæ•¸å€¼ ---\")\n",
    "    print(f\"å»ºè­°çš„ ğœ‡_Hn (mu_entropy): {mean_entropy:.4f}\")\n",
    "    print(f\"å»ºè­°çš„ Ïƒ_c (sigma_entropy): {std_entropy:.4f}  (é€™æ˜¯ä¸€å€‹å¥½çš„èµ·å§‹é»ï¼Œæ‚¨å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´)\")\n",
    "\n",
    "    # --- è¦–è¦ºåŒ– ---\n",
    "    print(\"\\næ­£åœ¨ç”Ÿæˆç†µå€¼åˆ†ä½ˆåœ–...\")\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(entropy_series, kde=True, bins=50)\n",
    "    plt.axvline(mean_entropy, color='r', linestyle='--', label=f'Mean: {mean_entropy:.2f}')\n",
    "    plt.axvline(mean_entropy + std_entropy, color='g', linestyle=':', label=f'+1 Std Dev: {mean_entropy + std_entropy:.2f}')\n",
    "    plt.axvline(mean_entropy - std_entropy, color='g', linestyle=':', label=f'-1 Std Dev: {mean_entropy - std_entropy:.2f}')\n",
    "    plt.title('äººé¡æ¼”å¥è³‡æ–™é›†çš„é¦™è¾²ç†µ (H_n) åˆ†ä½ˆ', fontsize=16)\n",
    "    plt.xlabel('Shannon Entropy', fontsize=12)\n",
    "    plt.ylabel('æ­Œæ›²æ•¸é‡ (Count)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"ipe_entropy_distribution.png\", dpi=300)\n",
    "    print(\"âœ… ç†µå€¼åˆ†ä½ˆåœ–å·²å„²å­˜ç‚º ipe_entropy_distribution.png\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # å®‰è£å¿…è¦çš„å‡½å¼åº«\n",
    "    # pip install pandas matplotlib seaborn tqdm\n",
    "    analyze_dataset_for_ipe_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å‡è¨­æ‚¨çš„ IpeCalculator é¡åˆ¥å„²å­˜åœ¨ evaluation/IPE.py\n",
    "from evaluation.IPE import IPECalculator \n",
    "\n",
    "\n",
    "VERSION_DISPLAY_NAMES = {\n",
    "    \"human\": \"Human\",\n",
    "    \"etude_e\": \"Etude Extractor\",\n",
    "    \"etude_d\": \"Etude Decoder\",\n",
    "    \"picogen\": \"PiCoGen\",\n",
    "    \"amtapc\": \"AMT-APC\",\n",
    "    \"music2midi\": \"Music2MIDI\"\n",
    "}\n",
    "\n",
    "def evaluate_models_with_ipe():\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ ¡æº–å¾Œçš„ IPE åƒæ•¸ï¼Œè©•ä¼° eval è³‡æ–™é›†ä¸­å„å€‹æ¨¡å‹çš„è¡¨ç¾ã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    eval_dir = \"./dataset/eval\"\n",
    "    metadata_path = os.path.join(eval_dir, \"metadata.json\")\n",
    "    versions = [\"cover\", \"picogen\", \"amtapc\", \"music2midi\", \"etude_e\", \"etude_d\"]\n",
    "    \n",
    "    # ä½¿ç”¨æ‚¨å¾ 4751 é¦–æ­Œæ›²ä¸­åˆ†æå‡ºçš„é»ƒé‡‘åƒæ•¸\n",
    "    EMPIRICAL_MU = 10.2402\n",
    "    EMPIRICAL_SIGMA = 0.7174\n",
    "    \n",
    "    # åˆå§‹åŒ–è¨ˆç®—å™¨\n",
    "    ipe_calculator = IPECalculator(\n",
    "        mu_entropy=EMPIRICAL_MU, \n",
    "        sigma_entropy=EMPIRICAL_SIGMA,\n",
    "        n_gram=5, \n",
    "        n_clusters=16 # ç¢ºä¿èˆ‡åˆ†ææ™‚çš„åƒæ•¸ä¸€è‡´\n",
    "    )\n",
    "\n",
    "    # --- 2. è®€å– metadata ä¸¦è¨ˆç®—åˆ†æ•¸ ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"âœ… æˆåŠŸè®€å– metadata.jsonï¼Œå°‡åˆ†æ {len(metadata)} é¦–æ­Œæ›²ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° metadata.json æª”æ¡ˆã€‚\")\n",
    "        return\n",
    "\n",
    "    results_list = []\n",
    "    for song_data in tqdm(metadata, desc=\"Evaluating Songs\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "\n",
    "        song_dir = os.path.join(eval_dir, dir_name)\n",
    "        \n",
    "        for version in versions:\n",
    "            midi_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "            if not os.path.exists(midi_path): continue\n",
    "\n",
    "            results = ipe_calculator.calculate_ipe(midi_path)\n",
    "            if \"error\" not in results:\n",
    "                results_list.append({\n",
    "                    \"song\": dir_name,\n",
    "                    \"version\": VERSION_DISPLAY_NAMES.get(version, version),\n",
    "                    \"ipe_score\": results[\"ipe_score\"],\n",
    "                    \"entropy\": results[\"shannon_entropy\"]\n",
    "                })\n",
    "\n",
    "    # --- 3. ä½¿ç”¨ Pandas é€²è¡Œçµ±è¨ˆåˆ†æ ---\n",
    "    if not results_list:\n",
    "        print(\"æœªè¨ˆç®—å‡ºä»»ä½•æœ‰æ•ˆåˆ†æ•¸ã€‚\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    print(\"\\n\\n--- å„ç‰ˆæœ¬ IPE åˆ†æ•¸çµ±è¨ˆæ‘˜è¦ ---\")\n",
    "    # æ ¹æ“šå¹³å‡åˆ†æ•¸é€²è¡Œæ’åº\n",
    "    summary = df.groupby('version')['ipe_score'].describe().sort_values('mean', ascending=False)\n",
    "    print(summary)\n",
    "    \n",
    "    print(\"\\n--- å„ç‰ˆæœ¬å¹³å‡ç†µå€¼ (èˆ‡ç†æƒ³å€¼ 10.8956 æ¯”è¼ƒ) ---\")\n",
    "    mean_entropy = df.groupby('version')['entropy'].mean().sort_values(ascending=False)\n",
    "    print(mean_entropy)\n",
    "\n",
    "    # --- 4. æ•¸æ“šè¦–è¦ºåŒ– ---\n",
    "    print(\"\\nğŸ¨ æ­£åœ¨ç”Ÿæˆåˆ†æ•¸åˆ†ä½ˆçš„ç®±å½¢åœ– (Box Plot)...\")\n",
    "    \n",
    "    # æ ¹æ“šå¹³å‡åˆ†å°ç‰ˆæœ¬é€²è¡Œæ’åºï¼Œè®“åœ–è¡¨æ›´æ¸…æ™°\n",
    "    order = summary.index \n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.boxplot(data=df, x='ipe_score', y='version', order=order, palette='viridis')\n",
    "    \n",
    "    plt.title('IPE Score Distribution', fontsize=18, pad=20)\n",
    "    plt.xlabel('IPE Score', fontsize=14)\n",
    "    plt.ylabel('Version', fontsize=14)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    output_image_path = \"ipe_evaluation_results.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"âœ… è©•ä¼°çµæœåœ–è¡¨å·²æˆåŠŸå„²å­˜è‡³: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_models_with_ipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å‡è¨­æ‚¨çš„ IpeCalculator é¡åˆ¥å„²å­˜åœ¨ evaluation/IPE.py\n",
    "from evaluation.IPE import IPECalculator \n",
    "\n",
    "def run_deep_dive_analysis():\n",
    "    \"\"\"\n",
    "    åŸ·è¡Œ IPE æŒ‡æ¨™çš„æ·±åº¦è¨ºæ–·åˆ†æï¼Œæ”¶é›†ä¸¦å‘ˆç¾ä¸­é–“æ•¸æ“šã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    eval_dir = \"./dataset/eval\"\n",
    "    metadata_path = os.path.join(eval_dir, \"metadata.json\")\n",
    "    versions = [\"cover\", \"music2midi\", \"etude_d\", \"etude_e\", \"picogen\", \"amtapc\"]\n",
    "    \n",
    "    # ä½¿ç”¨æ‚¨æ ¡æº–å¾Œçš„åƒæ•¸\n",
    "    EMPIRICAL_MU = 10.2402\n",
    "    EMPIRICAL_SIGMA = 0.7174\n",
    "    \n",
    "    ipe_calculator = IPECalculator(\n",
    "        mu_entropy=EMPIRICAL_MU, \n",
    "        sigma_entropy=EMPIRICAL_SIGMA,\n",
    "        n_gram=5, \n",
    "        n_clusters=16\n",
    "    )\n",
    "    \n",
    "    # ä¿®æ”¹ IPECalculator çš„ IOI æå–æ–¹æ³•ï¼Œä»¥å›å‚³ä¸­é–“æ•¸æ“š\n",
    "    original_get_ioi_sequence = ipe_calculator.get_ioi_sequence\n",
    "    def get_ioi_with_stats(midi_path: str):\n",
    "        # é€™æ˜¯ä¸€å€‹ wrapper å‡½å¼ï¼Œç”¨æ–¼æ•ç²é è™•ç†å‰å¾Œçš„æ•¸æ“š\n",
    "        try:\n",
    "            midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "            onsets = []\n",
    "            for instrument in midi_data.instruments:\n",
    "                if not instrument.is_drum:\n",
    "                    onsets.extend([note.start for note in instrument.notes])\n",
    "            if len(onsets) < 2: return None, {}\n",
    "            \n",
    "            unique_onsets = np.unique(onsets)\n",
    "            unique_onsets.sort()\n",
    "            if len(unique_onsets) < 2: return None, {}\n",
    "            \n",
    "            raw_ioi = np.diff(unique_onsets)\n",
    "            processed_ioi = ipe_calculator._process_raw_ioi(raw_ioi)\n",
    "            \n",
    "            stats = {\n",
    "                'raw_ioi_count': len(raw_ioi),\n",
    "                'processed_ioi_count': len(processed_ioi),\n",
    "                'filtered_percent': (1 - len(processed_ioi) / len(raw_ioi)) * 100 if len(raw_ioi) > 0 else 0,\n",
    "                'capped_count': np.sum(raw_ioi > ipe_calculator.max_ioi)\n",
    "            }\n",
    "            return processed_ioi, stats\n",
    "        except Exception:\n",
    "            return None, {}\n",
    "\n",
    "    ipe_calculator.get_ioi_sequence = get_ioi_with_stats # Monkey-patch a a method\n",
    "    \n",
    "    # --- 2. æ”¶é›†æ•¸æ“š ---\n",
    "    results_list = []\n",
    "    metadata = json.load(open(metadata_path, 'r', encoding='utf-8'))\n",
    "\n",
    "    for song_data in tqdm(metadata, desc=\"Deep Dive Analysis\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "        song_dir = os.path.join(eval_dir, dir_name)\n",
    "        \n",
    "        for version in versions:\n",
    "            midi_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "            if not os.path.exists(midi_path): continue\n",
    "            \n",
    "            # ä¿®æ”¹å¾Œçš„ ioi æå–æ–¹æ³•æœƒå›å‚³é¡å¤–çµ±è¨ˆæ•¸æ“š\n",
    "            ioi_sequence, io_stats = ipe_calculator.get_ioi_sequence(midi_path)\n",
    "            \n",
    "            if ioi_sequence is None or ioi_sequence.size == 0: continue\n",
    "\n",
    "            # ç¹¼çºŒè¨ˆç®—ç†µç­‰æŒ‡æ¨™\n",
    "            symbol_sequence = ipe_calculator.quantize_ioi_to_symbols(ioi_sequence)\n",
    "            if symbol_sequence.size == 0: continue\n",
    "            ngrams = ipe_calculator.get_ngrams_from_sequence(symbol_sequence, ipe_calculator.n_gram)\n",
    "            entropy = ipe_calculator.get_shannon_entropy(ngrams)\n",
    "            ipe_score = np.exp(-((entropy - ipe_calculator.mu_entropy)**2) / (2 * ipe_calculator.sigma_entropy**2))\n",
    "            \n",
    "            result_item = {\n",
    "                'song': dir_name, 'version': version, 'ipe_score': ipe_score, 'entropy': entropy\n",
    "            }\n",
    "            result_item.update(io_stats) # å°‡ IOI çµ±è¨ˆæ•¸æ“šåŠ å…¥çµæœ\n",
    "            results_list.append(result_item)\n",
    "\n",
    "    # --- 3. æ•¸æ“šåˆ†æèˆ‡å‘ˆç¾ ---\n",
    "    if not results_list:\n",
    "        print(\"æœªèƒ½æ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆæ•¸æ“šã€‚\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # è¨ºæ–·ä¸€ï¼šç†µå€¼åˆ†ä½ˆ\n",
    "    print(\"\\n\\n--- è¨ºæ–·ä¸€ï¼šå„ç‰ˆæœ¬ã€é¦™è¾²ç†µã€è©³ç´°çµ±è¨ˆ ---\")\n",
    "    entropy_stats = df.groupby('version')['entropy'].describe().sort_values('mean', ascending=False)\n",
    "    print(entropy_stats)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.kdeplot(data=df, x='entropy', hue='version', fill=True, alpha=0.5, palette='viridis')\n",
    "    plt.axvline(EMPIRICAL_MU, color='r', linestyle='--', label=f'Ideal Î¼: {EMPIRICAL_MU:.2f}')\n",
    "    plt.title('H Distribution', fontsize=16)\n",
    "    plt.xlabel('Shannon Entropy')\n",
    "    plt.legend()\n",
    "    plt.savefig('entropy_distribution_analysis.png', dpi=300)\n",
    "    print(\"âœ… ç†µå€¼åˆ†ä½ˆåœ–å·²å„²å­˜è‡³ entropy_distribution_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # è¨ºæ–·äºŒï¼šIOI é è™•ç†å½±éŸ¿\n",
    "    print(\"\\n\\n--- è¨ºæ–·äºŒï¼šIOI é è™•ç†å½±éŸ¿åˆ†æ (å¹³å‡å€¼) ---\")\n",
    "    processing_stats = df.groupby('version')[['filtered_percent', 'capped_count']].mean().sort_values('filtered_percent', ascending=False)\n",
    "    print(processing_stats)\n",
    "\n",
    "    # è¨ºæ–·ä¸‰ï¼šK-Means ç”¨è©ç¿’æ…£ (ä»¥åˆ†æ•¸å·®ç•°æœ€å¤§çš„ä¸€é¦–æ­Œç‚ºä¾‹)\n",
    "    print(\"\\n\\n--- è¨ºæ–·ä¸‰ï¼šç¯€å¥ç¬¦è™Ÿä½¿ç”¨é »ç‡æ¯”è¼ƒ (ç¯„ä¾‹) ---\")\n",
    "    # æ‰¾åˆ° cover å’Œ music2midi åˆ†æ•¸å·®ç•°æœ€å¤§çš„ä¸€é¦–æ­Œ\n",
    "    pivot_df = df.pivot(index='song', columns='version', values='ipe_score')\n",
    "    pivot_df['diff'] = (pivot_df['music2midi'] - pivot_df['cover']).abs()\n",
    "    sample_song_dir = pivot_df.nlargest(1, 'diff').index[0]\n",
    "    print(f\"ä»¥åˆ†æ•¸å·®ç•°æœ€å¤§çš„æ­Œæ›² '{sample_song_dir}' ç‚ºä¾‹é€²è¡Œåˆ†æ:\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "    \n",
    "    for i, version in enumerate(['cover', 'music2midi']):\n",
    "        midi_path = os.path.join(eval_dir, sample_song_dir, f\"{version}.mid\")\n",
    "        ioi_sequence, _ = ipe_calculator.get_ioi_sequence(midi_path)\n",
    "        symbol_sequence = ipe_calculator.quantize_ioi_to_symbols(ioi_sequence)\n",
    "        \n",
    "        if symbol_sequence.size > 0:\n",
    "            sns.histplot(symbol_sequence, ax=axs[i], bins=ipe_calculator.n_clusters, kde=False)\n",
    "            axs[i].set_title(f\"Symbol Usage - {version} @ {sample_song_dir}\")\n",
    "            axs[i].set_xticks(range(ipe_calculator.n_clusters))\n",
    "\n",
    "    plt.suptitle(\"Freq\", fontsize=16)\n",
    "    plt.savefig('symbol_usage_analysis.png', dpi=300)\n",
    "    print(\"âœ… ç¯€å¥ç¬¦è™Ÿä½¿ç”¨é »ç‡æ¯”è¼ƒåœ–å·²å„²å­˜è‡³ symbol_usage_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # monkey-patch a class a method a little bit\n",
    "    from evaluation.IPE import pretty_midi\n",
    "    run_deep_dive_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bac921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# å‡è¨­æ‚¨çš„ RGCCalculator é¡åˆ¥ä½æ–¼æ­¤è™•\n",
    "from evaluation import RGCCalculator\n",
    "\n",
    "def get_genre_from_dirname(dir_name: str) -> str:\n",
    "    \"\"\"\n",
    "    æ ¹æ“šç›®éŒ„åç¨±æ¨æ–·éŸ³æ¨‚é¡å‹ã€‚\n",
    "    \"\"\"\n",
    "    dir_name_upper = dir_name.upper()\n",
    "    if \"CPOP\" in dir_name_upper:\n",
    "        return \"CPOP\"\n",
    "    elif \"JPOP\" in dir_name_upper:\n",
    "        return \"JPOP\"\n",
    "    elif \"KPOP\" in dir_name_upper:\n",
    "        return \"KPOP\"\n",
    "    elif \"WESTERN\" in dir_name_upper:\n",
    "        return \"WESTERN\"\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "def analyze_rgc_by_genre_and_overall():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½å¼ï¼šè¨ˆç®—æ‰€æœ‰æ­Œæ›²å„ç‰ˆæœ¬çš„ RGC åˆ†æ•¸ï¼Œå…ˆæŒ‰éŸ³æ¨‚é¡å‹åˆ†æï¼Œæœ€å¾Œå†é€²è¡Œç¸½é«”åˆ†æã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    eval_dir = \"./dataset/eval\"\n",
    "    metadata_path = os.path.join(eval_dir, \"metadata.json\")\n",
    "    versions = [\"cover\", \"picogen\", \"etude_d\", \"music2midi\", \"amtapc\", \"etude_e\"]\n",
    "    genres_to_analyze = [\"CPOP\", \"JPOP\", \"KPOP\", \"WESTERN\"]\n",
    "    \n",
    "    VERSION_DISPLAY_NAMES = {\n",
    "        \"cover\": \"Human\",\n",
    "        \"picogen\": \"PiCoGen\",\n",
    "        \"etude_d\": \"Etude Decoder\",\n",
    "        \"music2midi\": \"Music2MIDI\",\n",
    "        \"amtapc\": \"AMT-APC\",\n",
    "        \"etude_e\": \"Etude Extractor\"\n",
    "    }\n",
    "    \n",
    "    # ä½¿ç”¨æ‚¨è¦ºå¾—æ•ˆæœæœ€å¥½çš„å¾®èª¿åƒæ•¸\n",
    "    rgc_calc = RGCCalculator(\n",
    "        reasonable_bpm_range=(60, 240),\n",
    "        tau_falloff_sigma=0.03,\n",
    "        lambda_grid_fit=10.0\n",
    "    )\n",
    "\n",
    "    # --- 2. ä¸€æ¬¡æ€§éæ­·æ‰€æœ‰æ­Œæ›²ä¸¦è¨ˆç®—åˆ†æ•¸ ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"âœ… æˆåŠŸè®€å– metadata.jsonï¼Œå°‡åˆ†æ {len(metadata)} é¦–æ­Œæ›²ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° metadata.json æª”æ¡ˆæ–¼ {metadata_path}\")\n",
    "        return\n",
    "        \n",
    "    results_list = []\n",
    "    for song_data in tqdm(metadata, desc=\"Calculating all RGC scores\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "        \n",
    "        genre = get_genre_from_dirname(dir_name)\n",
    "        song_dir = os.path.join(eval_dir, dir_name)\n",
    "        \n",
    "        for version in versions:\n",
    "            midi_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "            if not os.path.exists(midi_path): continue\n",
    "            \n",
    "            results = rgc_calc.calculate_rgc(midi_path)\n",
    "            if \"error\" not in results:\n",
    "                results['song'] = dir_name\n",
    "                results['version'] = version\n",
    "                results['genre'] = genre\n",
    "                results_list.append(results)\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"æœªèƒ½è¨ˆç®—å‡ºä»»ä½•æœ‰æ•ˆåˆ†æ•¸ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Šã€‚\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.DataFrame(results_list)\n",
    "    df_all['display_name'] = df_all['version'].map(VERSION_DISPLAY_NAMES)\n",
    "    \n",
    "    print(\"\\nâœ… æ‰€æœ‰æ­Œæ›²åˆ†æ•¸è¨ˆç®—å®Œç•¢ï¼Œé–‹å§‹æŒ‰é¡å‹é€²è¡Œåˆ†æ...\")\n",
    "\n",
    "    # --- 3. æŒ‰éŸ³æ¨‚é¡å‹éæ­·ï¼Œåˆ†åˆ¥é€²è¡Œåˆ†æèˆ‡å‘ˆç¾ ---\n",
    "    for genre in genres_to_analyze:\n",
    "        print(f\"\\n\\n{'='*25} åˆ†æå ±å‘Š: {genre} {'='*25}\")\n",
    "        \n",
    "        df_genre = df_all[df_all['genre'] == genre].copy()\n",
    "        \n",
    "        if df_genre.empty:\n",
    "            print(f\"åœ¨è³‡æ–™é›†ä¸­æ‰¾ä¸åˆ°é¡å‹ç‚º '{genre}' çš„æ­Œæ›²ï¼Œå·²è·³éã€‚\")\n",
    "            continue\n",
    "            \n",
    "        # ... (æ­¤è™•çœç•¥äº†åˆ†é …å ±å‘Šçš„ print å’Œç¹ªåœ–é‚è¼¯ï¼Œèˆ‡æ‚¨æä¾›çš„ä¸€è‡´) ...\n",
    "        print(f\"\\n--- {genre} é¡å‹ RGC åˆ†æ•¸çµ±è¨ˆ ---\")\n",
    "        summary = df_genre.groupby('display_name')['rgc_score'].describe().sort_values('mean', ascending=False)\n",
    "        print(summary)\n",
    "        \n",
    "        print(f\"\\n--- {genre} é¡å‹ã€åŸºæœ¬ç¯€æ‹å–®ä½ Ï„ã€çµ±è¨ˆ (ç§’) ---\")\n",
    "        tau_summary = df_genre.groupby('display_name')['inferred_tau'].describe()\n",
    "        print(tau_summary[['mean', 'std']])\n",
    "        \n",
    "        order = summary.index \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        sns.boxplot(data=df_genre, x='rgc_score', y='display_name', order=order, palette='plasma')\n",
    "        ax.set_title(f'RGC Score Distribution for {genre}', fontsize=18, pad=20)\n",
    "        ax.set_xlabel('RGC Score', fontsize=14)\n",
    "        ax.set_ylabel('Version', fontsize=14)\n",
    "        ax.set_xlim(-0.05, max(df_genre['rgc_score'].max() * 1.1, 0.8) if not df_genre.empty else 1.0)\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        output_image_path = f\"rgc_evaluation_{genre}.png\"\n",
    "        plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"âœ… {genre} é¡å‹è©•ä¼°åœ–è¡¨å·²æˆåŠŸå„²å­˜è‡³: {output_image_path}\")\n",
    "\n",
    "    # --- ã€é—œéµä¿®æ”¹ã€‘æ–°å¢ç¸½é«”åˆ†æå ±å‘Š ---\n",
    "    print(f\"\\n\\n{'='*25} ç¸½é«”åˆ†æå ±å‘Š: ALL GENRES ({len(metadata)}é¦–æ­Œæ›²) {'='*25}\")\n",
    "\n",
    "    print(\"\\n--- ç¸½é«” RGC åˆ†æ•¸çµ±è¨ˆ ---\")\n",
    "    summary_all = df_all.groupby('display_name')['rgc_score'].describe().sort_values('mean', ascending=False)\n",
    "    print(summary_all)\n",
    "\n",
    "    print(\"\\n--- ç¸½é«”ã€åŸºæœ¬ç¯€æ‹å–®ä½ Ï„ã€çµ±è¨ˆ (ç§’) ---\")\n",
    "    tau_summary_all = df_all.groupby('display_name')['inferred_tau'].describe()\n",
    "    print(tau_summary_all[['mean', 'std']])\n",
    "    \n",
    "    # --- ç¹ªè£½ç¸½é«”åœ–è¡¨ ---\n",
    "    print(\"\\nğŸ¨ æ­£åœ¨ç”Ÿæˆç¸½é«”åˆ†æ•¸åˆ†ä½ˆåœ–...\")\n",
    "    \n",
    "    order_all = summary_all.index \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    sns.boxplot(data=df_all, x='rgc_score', y='display_name', order=order_all, palette='viridis')\n",
    "    \n",
    "    ax.set_title('Overall RGC Score Distribution (All Genres)', fontsize=18, pad=20)\n",
    "    ax.set_xlabel('RGC Score', fontsize=14)\n",
    "    ax.set_ylabel('Version', fontsize=14)\n",
    "    ax.set_xlim(-0.05, max(df_all['rgc_score'].max() * 1.1, 0.8))\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    output_image_path_overall = \"rgc_evaluation_OVERALL.png\"\n",
    "    plt.savefig(output_image_path_overall, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"âœ… ç¸½é«”è©•ä¼°åœ–è¡¨å·²æˆåŠŸå„²å­˜è‡³: {output_image_path_overall}\")\n",
    "    \n",
    "    print(\"\\n\\nğŸ‰ æ‰€æœ‰é¡å‹çš„åˆ†æèˆ‡è¦–è¦ºåŒ–å‡å·²å®Œæˆï¼\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ç¢ºä¿æ‚¨çš„ RGCCalculator é¡åˆ¥å®šç¾©åœ¨ evaluation.py ä¸­ï¼Œæˆ–ä¿®æ”¹ä¸‹é¢çš„ import è·¯å¾‘\n",
    "    from evaluation import RGCCalculator\n",
    "    analyze_rgc_by_genre_and_overall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# å‡è¨­æ‚¨çš„ RGCCalculator é¡åˆ¥ä½æ–¼æ­¤è™•\n",
    "from evaluation import RGCCalculator\n",
    "from evaluation import RCCalculator\n",
    "\n",
    "\n",
    "def get_genre_from_dirname(dir_name: str) -> str:\n",
    "    \"\"\"\n",
    "    æ ¹æ“šç›®éŒ„åç¨±æ¨æ–·éŸ³æ¨‚é¡å‹ã€‚\n",
    "    \"\"\"\n",
    "    dir_name_upper = dir_name.upper()\n",
    "    if \"CPOP\" in dir_name_upper:\n",
    "        return \"CPOP\"\n",
    "    elif \"JPOP\" in dir_name_upper:\n",
    "        return \"JPOP\"\n",
    "    elif \"KPOP\" in dir_name_upper:\n",
    "        return \"KPOP\"\n",
    "    elif \"WESTERN\" in dir_name_upper:\n",
    "        return \"WESTERN\"\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "def analyze_rgc_by_genre():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½å¼ï¼šè¨ˆç®—æ‰€æœ‰æ­Œæ›²å„ç‰ˆæœ¬çš„ RGC åˆ†æ•¸ï¼Œç„¶å¾ŒæŒ‰éŸ³æ¨‚é¡å‹åˆ†åˆ¥é€²è¡Œåˆ†æèˆ‡è¦–è¦ºåŒ–ã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    eval_dir = \"./dataset/eval\"\n",
    "    metadata_path = os.path.join(eval_dir, \"metadata.json\")\n",
    "    versions = [\"cover\", \"picogen\", \"etude_d\", \"etude_d_d\", \"music2midi\", \"amtapc\", \"etude_e\"]\n",
    "    genres_to_analyze = [\"CPOP\", \"JPOP\", \"KPOP\", \"WESTERN\"]\n",
    "    \n",
    "    # ä½¿ç”¨æ‚¨è¦ºå¾—æ•ˆæœæœ€å¥½çš„å¾®èª¿åƒæ•¸\n",
    "    LAMBDA_GRID_FIT = 10.0\n",
    "    \n",
    "    rc_calc = RCCalculator(\n",
    "        top_k=8,\n",
    "        lambda_grid_fit=LAMBDA_GRID_FIT\n",
    "    )\n",
    "\n",
    "    # --- 2. ä¸€æ¬¡æ€§éæ­·æ‰€æœ‰æ­Œæ›²ä¸¦è¨ˆç®—åˆ†æ•¸ ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"âœ… æˆåŠŸè®€å– metadata.jsonï¼Œå°‡åˆ†æ {len(metadata)} é¦–æ­Œæ›²ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° metadata.json æª”æ¡ˆæ–¼ {metadata_path}\")\n",
    "        return\n",
    "        \n",
    "    results_list = []\n",
    "    for song_data in tqdm(metadata, desc=\"Calculating all RC scores\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "        \n",
    "        genre = get_genre_from_dirname(dir_name)\n",
    "        song_dir = os.path.join(eval_dir, dir_name)\n",
    "        \n",
    "        for version in versions:\n",
    "            midi_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "            if not os.path.exists(midi_path): continue\n",
    "            \n",
    "            results = rc_calc.calculate_rc(midi_path)\n",
    "            if \"error\" not in results:\n",
    "                results['song'] = dir_name\n",
    "                results['version'] = version\n",
    "                results['genre'] = genre # å°‡é¡å‹è³‡è¨ŠåŠ å…¥çµæœ\n",
    "                results_list.append(results)\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"æœªèƒ½è¨ˆç®—å‡ºä»»ä½•æœ‰æ•ˆåˆ†æ•¸ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Šã€‚\")\n",
    "        return\n",
    "\n",
    "    # å»ºç«‹åŒ…å«æ‰€æœ‰çµæœçš„ä¸» DataFrame\n",
    "    df_all = pd.DataFrame(results_list)\n",
    "    print(\"\\nâœ… æ‰€æœ‰æ­Œæ›²åˆ†æ•¸è¨ˆç®—å®Œç•¢ï¼Œé–‹å§‹æŒ‰é¡å‹é€²è¡Œåˆ†æ...\")\n",
    "\n",
    "    # --- 3. ã€é—œéµä¿®æ”¹ã€‘æŒ‰éŸ³æ¨‚é¡å‹éæ­·ï¼Œåˆ†åˆ¥é€²è¡Œåˆ†æèˆ‡å‘ˆç¾ ---\n",
    "    for genre in genres_to_analyze:\n",
    "        print(f\"\\n\\n{'='*25} åˆ†æå ±å‘Š: {genre} {'='*25}\")\n",
    "        \n",
    "        # ç¯©é¸å‡ºç•¶å‰é¡å‹çš„æ•¸æ“š\n",
    "        df_genre = df_all[df_all['genre'] == genre].copy()\n",
    "        \n",
    "        if df_genre.empty:\n",
    "            print(f\"åœ¨è³‡æ–™é›†ä¸­æ‰¾ä¸åˆ°é¡å‹ç‚º '{genre}' çš„æ­Œæ›²ï¼Œå·²è·³éã€‚\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- {genre} é¡å‹ RC åˆ†æ•¸çµ±è¨ˆ ---\")\n",
    "        summary = df_genre.groupby('version')['rc_score'].describe().sort_values('mean', ascending=False)\n",
    "        print(summary)\n",
    "        \n",
    "        # print(f\"\\n--- {genre} é¡å‹ã€åŸºæœ¬ç¯€æ‹å–®ä½ Ï„ã€çµ±è¨ˆ (ç§’) ---\")\n",
    "        # tau_summary = df_genre.groupby('version')['inferred_tau'].describe()\n",
    "        # print(tau_summary[['mean', 'std']])\n",
    "        \n",
    "        # --- ç¹ªè£½è©²é¡å‹çš„åœ–è¡¨ ---\n",
    "        print(f\"\\nğŸ¨ æ­£åœ¨ç”Ÿæˆ {genre} é¡å‹çš„åˆ†æ•¸åˆ†ä½ˆåœ–...\")\n",
    "        \n",
    "        order = summary.index \n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, ax = plt.subplots(figsize=(12, 8)) # ç‚ºæ¯å€‹é¡å‹å»ºç«‹æ–°çš„åœ–è¡¨\n",
    "        \n",
    "        sns.boxplot(data=df_genre, x='rc_score', y='version', order=order, palette='plasma')\n",
    "        \n",
    "        ax.set_title(f'RGC Score Distribution for {genre}', fontsize=18, pad=20)\n",
    "        ax.set_xlabel('RGC Score', fontsize=14)\n",
    "        ax.set_ylabel('Version', fontsize=14)\n",
    "        ax.set_xlim(-0.05, max(df_genre['rc_score'].max() * 1.1, 0.8))\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        output_image_path = f\"rgc_evaluation_{genre}.png\"\n",
    "        plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig) # é—œé–‰ç•¶å‰çš„åœ–è¡¨ï¼Œé¿å…åœ¨ä¸‹ä¸€æ¬¡è¿´åœˆä¸­é‡ç–Š\n",
    "        \n",
    "        print(f\"âœ… {genre} é¡å‹è©•ä¼°åœ–è¡¨å·²æˆåŠŸå„²å­˜è‡³: {output_image_path}\")\n",
    "\n",
    "    print(\"\\n\\nğŸ‰ æ‰€æœ‰é¡å‹çš„åˆ†æèˆ‡è¦–è¦ºåŒ–å‡å·²å®Œæˆï¼\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_rgc_by_genre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f13da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Penalty Weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:28<00:00, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== WPD-N æ¬Šé‡æ•æ„Ÿåº¦åˆ†æå ±å‘Š ====================\n",
      "\n",
      "--- å ±å‘Šä¸€ï¼šå„ç‰ˆæœ¬åœ¨ä¸åŒæ¬Šé‡ä¸‹çš„å¹³å‡èª¤å·®å€¼ (è¶Šä½è¶Šå¥½) ---\n",
      "penalty_weight                0.00      0.05      0.10      0.50\n",
      "version                                                         \n",
      "AMT-APC                   0.086916  1.062312  2.037707  9.840872\n",
      "Etude Decoder - Default   0.211236  0.701990  1.192744  5.118775\n",
      "Etude Decoder - Prompted  0.233434  0.829003  1.424573  6.189125\n",
      "Etude Extractor           0.118497  0.911008  1.703518  8.043605\n",
      "Human                     0.489353  0.924388  1.359422  4.839699\n",
      "Music2MIDI                0.183201  0.487121  0.791041  3.222401\n",
      "PiCoGen                   1.001302  1.386048  1.770795  4.848767\n",
      "\n",
      "\n",
      "--- å ±å‘ŠäºŒï¼šå„ç‰ˆæœ¬åœ¨ä¸åŒæ¬Šé‡ä¸‹çš„æ’åè®ŠåŒ– (1=æœ€å¥½) ---\n",
      "penalty_weight            0.00  0.05  0.10  0.50\n",
      "version                                         \n",
      "AMT-APC                      1     6     7     7\n",
      "Etude Decoder - Default      4     2     2     4\n",
      "Etude Decoder - Prompted     5     3     4     5\n",
      "Etude Extractor              2     4     5     6\n",
      "Human                        6     5     3     2\n",
      "Music2MIDI                   3     1     1     1\n",
      "PiCoGen                      7     7     6     3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluation import WPDCalculator\n",
    "\n",
    "def test_density_penalty_weights():\n",
    "    \"\"\"\n",
    "    æ¸¬è©¦ä¸åŒçš„ density_penalty_weight å€¼å°æœ€çµ‚æ¨¡å‹æ’åçš„å½±éŸ¿ã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    EVAL_DIR = \"./dataset/eval\"\n",
    "    METADATA_PATH = os.path.join(EVAL_DIR, \"metadata.json\")\n",
    "    VERSIONS = [\"cover\", \"etude_e\", \"etude_d\", \"etude_d_d\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    VERSION_DISPLAY_NAMES = {\n",
    "        \"cover\": \"Human\", \"etude_e\": \"Etude Extractor\", \"etude_d_d\": \"Etude Decoder - Default\",\n",
    "        \"etude_d\": \"Etude Decoder - Prompted\", \"picogen\": \"PiCoGen\", \"amtapc\": \"AMT-APC\", \"music2midi\": \"Music2MIDI\"\n",
    "    }\n",
    "    \n",
    "    # ã€é—œéµã€‘è¨­å®šä¸€çµ„æ‚¨æƒ³è¦æ¸¬è©¦çš„æ¬Šé‡å€¼\n",
    "    weights_to_test = [0, 0.05, 0.1, 0.5]\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    try:\n",
    "        with open(METADATA_PATH, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° metadata.json æª”æ¡ˆæ–¼ {METADATA_PATH}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. éæ­·æ‰€æœ‰æ¬Šé‡å’Œæ‰€æœ‰æ­Œæ›² ---\n",
    "    for weight in tqdm(weights_to_test, desc=\"Testing Penalty Weights\"):\n",
    "        # ç‚ºæ¯å€‹æ¬Šé‡å¯¦ä¾‹åŒ–ä¸€å€‹æ–°çš„è¨ˆç®—å™¨\n",
    "        wpd_calc = WPDCalculator(\n",
    "            subsample_step=1,\n",
    "            trim_seconds=10,\n",
    "            density_penalty_weight=weight\n",
    "        )\n",
    "        \n",
    "        for song_data in metadata:\n",
    "            dir_name = song_data.get(\"dir_name\")\n",
    "            if not dir_name: continue\n",
    "            \n",
    "            song_dir = os.path.join(EVAL_DIR, dir_name)\n",
    "            origin_wav_path = os.path.join(song_dir, \"origin.wav\")\n",
    "            if not os.path.exists(origin_wav_path): continue\n",
    "            \n",
    "            for version in VERSIONS:\n",
    "                # WPD-N éœ€è¦ .mid æˆ– .json ä¾†è¨ˆç®—éŸ³ç¬¦å¯†åº¦\n",
    "                # æˆ‘å€‘å‡è¨­æª”åèˆ‡ .wav å°æ‡‰\n",
    "                file_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "                if not os.path.exists(file_path):\n",
    "                     file_path = os.path.join(song_dir, f\"{version}.json\")\n",
    "                if not os.path.exists(file_path): continue\n",
    "\n",
    "                wav_path = os.path.join(song_dir, f\"{version}.wav\")\n",
    "                if not os.path.exists(wav_path): continue\n",
    "                \n",
    "                results = wpd_calc.calculate_wpd(origin_wav_path, file_path, song_dir)\n",
    "                if \"error\" not in results:\n",
    "                    results['version'] = VERSION_DISPLAY_NAMES.get(version, version)\n",
    "                    results['penalty_weight'] = weight\n",
    "                    all_results.append(results)\n",
    "\n",
    "    # --- 3. ä½¿ç”¨ Pandas é€²è¡Œåˆ†æèˆ‡å ±å‘Š ---\n",
    "    if not all_results:\n",
    "        print(\"æœªèƒ½è¨ˆç®—å‡ºä»»ä½•æœ‰æ•ˆåˆ†æ•¸ã€‚\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*20 + \" WPD-N æ¬Šé‡æ•æ„Ÿåº¦åˆ†æå ±å‘Š \" + \"=\"*20)\n",
    "\n",
    "    # å ±å‘Šä¸€ï¼šä¸åŒæ¬Šé‡ä¸‹çš„å¹³å‡èª¤å·®å€¼\n",
    "    print(\"\\n--- å ±å‘Šä¸€ï¼šå„ç‰ˆæœ¬åœ¨ä¸åŒæ¬Šé‡ä¸‹çš„å¹³å‡èª¤å·®å€¼ (è¶Šä½è¶Šå¥½) ---\")\n",
    "    pivot_scores = df.pivot_table(\n",
    "        index='version', \n",
    "        columns='penalty_weight', \n",
    "        values='wpd_score',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    print(pivot_scores)\n",
    "    \n",
    "    # å ±å‘ŠäºŒï¼šä¸åŒæ¬Šé‡ä¸‹çš„æ¨¡å‹æ’å\n",
    "    print(\"\\n\\n--- å ±å‘ŠäºŒï¼šå„ç‰ˆæœ¬åœ¨ä¸åŒæ¬Šé‡ä¸‹çš„æ’åè®ŠåŒ– (1=æœ€å¥½) ---\")\n",
    "    pivot_ranks = pivot_scores.rank(axis=0, method='min').astype(int)\n",
    "    print(pivot_ranks)\n",
    "\n",
    "test_density_penalty_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
