{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa20ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from corpus import Synchronizer\n",
    "\n",
    "def calculate_wps(origin_path: str, cover_path: str, song_dir: str, lambda_wps: float = 1.0) -> float:\n",
    "    try:\n",
    "        s = Synchronizer()\n",
    "        wp = s.get_wp(origin_path, cover_path, song_dir)\n",
    "        \n",
    "        t_cover = s.t1\n",
    "        t_orig = s.t2\n",
    "        \n",
    "        if t_cover is None or t_orig is None:\n",
    "            raise ValueError(\"時間戳序列未能成功在 Synchronizer 物件中生成。\")\n",
    "\n",
    "        wp_int = wp.astype(int)\n",
    "        path_t_cover = t_cover[wp_int[0]]\n",
    "        path_t_orig = t_orig[wp_int[1]]\n",
    "\n",
    "        coeffs = np.polyfit(path_t_cover, path_t_orig, 1)\n",
    "        a, b = coeffs[0], coeffs[1]\n",
    "\n",
    "        t_orig_predicted = a * path_t_cover + b\n",
    "        deviation = path_t_orig - t_orig_predicted\n",
    "        sigma_dev = np.std(deviation)\n",
    "        wps_score = np.exp(-lambda_wps * sigma_dev)\n",
    "        return wps_score\n",
    "    except Exception as e:\n",
    "        print(f\"計算 wps 分數時發生錯誤：{e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def analyze_and_visualize_scores():\n",
    "    \"\"\"\n",
    "    主函式：計算所有歌曲各版本的 wps 分數，計算平均值，並進行視覺化。\n",
    "    \"\"\"\n",
    "    # --- 1. 設定 ---\n",
    "    base_dir = os.path.join(\".\", \"dataset\", \"eval\")\n",
    "    metadata_path = os.path.join(base_dir, \"metadata.json\")\n",
    "    origin_filename = \"origin.wav\"\n",
    "    versions = [\"human\", \"etude_e\", \"etude_d\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    lambda_val = 0.5\n",
    "    \n",
    "    # 用於儲存所有分數的字典\n",
    "    scores_by_version = {v: [] for v in versions}\n",
    "\n",
    "    # --- 2. 讀取 metadata 並計算分數 ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        # print(f\"✅ 成功讀取 metadata.json，共找到 {len(metadata)} 首歌曲。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 錯誤：找不到 metadata.json 檔案。\")\n",
    "        return\n",
    "\n",
    "    for i, song_data in enumerate(metadata):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name:\n",
    "            continue\n",
    "\n",
    "        song_dir = os.path.join(base_dir, dir_name)\n",
    "        # print(f\"\\n🎵 正在處理歌曲: {song_dir} ({i+1}/{len(metadata)})\")\n",
    "\n",
    "        origin_path = os.path.join(song_dir, origin_filename)\n",
    "        if not os.path.exists(origin_path):\n",
    "            print(f\"  ↪️ 已跳過 (找不到 origin.wav), {origin_path}\")\n",
    "            continue\n",
    "\n",
    "        for v in versions:\n",
    "            cover_path = os.path.join(song_dir, f\"{v}.wav\")\n",
    "            if not os.path.exists(cover_path):\n",
    "                print(f\"  ↪️ 已跳過版本 '{v}' (找不到 {v}.wav) {song_dir}\")\n",
    "                continue\n",
    "            \n",
    "            # 計算分數\n",
    "            score = calculate_wps(origin_path, cover_path, song_dir, lambda_wps=lambda_val)\n",
    "            if score > 0:\n",
    "                # print(f\"  📊 版本 '{v}' 的 wps 分數: {score:.4f}\")\n",
    "                scores_by_version[v].append(score)\n",
    "\n",
    "    # --- 3. 計算並打印平均分數 ---\n",
    "    print(\"\\n\\n--- 平均分數統計 ---\")\n",
    "    average_scores = {}\n",
    "    for version, scores in scores_by_version.items():\n",
    "        if scores:\n",
    "            avg_score = np.mean(scores)\n",
    "            average_scores[version] = avg_score\n",
    "            print(f\"版本 {version:<12}: 平均 wps 分數 = {avg_score:.4f} (基於 {len(scores)} 個樣本)\")\n",
    "        else:\n",
    "            print(f\"版本 {version:<12}: 無有效分數可供計算。\")\n",
    "    \n",
    "    # --- 4. 數據視覺化 ---\n",
    "    print(\"\\n🎨 正在生成分數分佈圖...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # 為每個版本繪製一條 KDE 曲線\n",
    "    for version, scores in scores_by_version.items():\n",
    "        if scores:\n",
    "            sns.kdeplot(scores, label=version, fill=True, alpha=0.5, ax=ax, lw=2.5)\n",
    "\n",
    "    ax.set_title('WPS Score Distribution by Version', fontsize=16, pad=20)\n",
    "    ax.set_xlabel('WPS Score', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_xlim(0, 1.05)\n",
    "    ax.legend(title='Version', fontsize=10)\n",
    "    \n",
    "    # 儲存圖表\n",
    "    output_image_path = \"wps_score_distribution.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"✅ 分數分佈圖已成功儲存至: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_and_visualize_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b11cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假設您的 Synchronizer 位於此處\n",
    "from corpus import Synchronizer \n",
    "\n",
    "# 【函式已加入 trim_seconds 參數】\n",
    "def calculate_nwpd(origin_path: str, cover_path: str, song_dir: str, \n",
    "                   lambda_nwpd: float = 1.0, subsample_step: int = 1, \n",
    "                   trim_seconds: float = 0) -> float:\n",
    "    \"\"\"\n",
    "    計算正規化路徑偏差 (NWPD) 分數。\n",
    "\n",
    "    Args:\n",
    "        ... (原有參數) ...\n",
    "        subsample_step (int): 對齊路徑的降採樣步長。\n",
    "        trim_seconds (float): 從路徑的頭尾各裁去幾秒不參與計算。設為 0 表示不裁切。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(subsample_step, int) or subsample_step < 1:\n",
    "            raise ValueError(\"subsample_step 必須是 >= 1 的整數。\")\n",
    "        if not isinstance(trim_seconds, (int, float)) or trim_seconds < 0:\n",
    "            raise ValueError(\"trim_seconds 必須是 >= 0 的數字。\")\n",
    "\n",
    "        s = Synchronizer()\n",
    "        wp = s.get_wp(origin_path, cover_path, song_dir)\n",
    "        \n",
    "        t_cover = s.t1\n",
    "        t_orig = s.t2\n",
    "        \n",
    "        if t_cover is None or t_orig is None:\n",
    "            raise ValueError(\"時間戳序列未能成功在 Synchronizer 物件中生成。\")\n",
    "\n",
    "        wp_int = wp.astype(int)\n",
    "        \n",
    "        # 降採樣\n",
    "        wp_to_process = wp_int[:, ::subsample_step] if subsample_step > 1 else wp_int\n",
    "        \n",
    "        if wp_to_process.shape[1] < 10: # 點太少則不進行後續處理\n",
    "            return 0.0\n",
    "\n",
    "        path_t_cover = t_cover[wp_to_process[0]]\n",
    "        path_t_orig = t_orig[wp_to_process[1]]\n",
    "\n",
    "        # --- 【關鍵修改 #1】根據秒數裁切對齊路徑 ---\n",
    "        if trim_seconds > 0:\n",
    "            total_duration = path_t_orig[-1]\n",
    "            # 確保樂曲長度足夠進行裁切\n",
    "            if total_duration > (2 * trim_seconds):\n",
    "                start_time = trim_seconds\n",
    "                end_time = total_duration - trim_seconds\n",
    "                \n",
    "                mask = (path_t_orig >= start_time) & (path_t_orig <= end_time)\n",
    "                \n",
    "                # 應用遮罩，並確保裁切後仍有足夠的點\n",
    "                if np.sum(mask) > 10:\n",
    "                    path_t_cover = path_t_cover[mask]\n",
    "                    path_t_orig = path_t_orig[mask]\n",
    "        # ---\n",
    "\n",
    "        coeffs = np.polyfit(path_t_cover, path_t_orig, 1)\n",
    "        a, b = coeffs[0], coeffs[1]\n",
    "\n",
    "        t_orig_predicted = a * path_t_cover + b\n",
    "        deviation = path_t_orig - t_orig_predicted\n",
    "        sigma_dev = np.std(deviation)\n",
    "        nwpd_score = np.exp(-lambda_nwpd * sigma_dev)\n",
    "        return nwpd_score\n",
    "    except Exception as e:\n",
    "        print(f\"計算 NWPD 分數時發生錯誤：{e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def analyze_and_visualize_scores():\n",
    "    \"\"\"\n",
    "    主函式：計算所有歌曲各版本的 NWPD 分數，計算平均值，並進行視覺化。\n",
    "    \"\"\"\n",
    "    # --- 1. 設定 ---\n",
    "    base_dir = os.path.join(\".\", \"dataset\", \"eval\")\n",
    "    metadata_path = os.path.join(base_dir, \"metadata.json\")\n",
    "    origin_filename = \"origin.wav\"\n",
    "    versions = [\"human\", \"etude_e\", \"etude_d\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    lambda_val = 0.5\n",
    "    \n",
    "    # 【新增】設定要裁切的頭尾秒數，設為 0 表示不裁切\n",
    "    TRIM_SECONDS = 10\n",
    "    \n",
    "    # 設定降採樣步長，1 表示不進行降採樣\n",
    "    SUBSAMPLE_STEP = 10 \n",
    "    \n",
    "    # 設定用於圖表和報告的顯示名稱\n",
    "    VERSION_DISPLAY_NAMES = {\n",
    "        \"human\": \"Human\",\n",
    "        \"etude_e\": \"Etude Extractor\",\n",
    "        \"etude_d\": \"Etude Decoder\",\n",
    "        \"picogen\": \"PiCoGen\",\n",
    "        \"amtapc\": \"AMT-APC\",\n",
    "        \"music2midi\": \"Music2MIDI\"\n",
    "    }\n",
    "    \n",
    "    results_list = []\n",
    "\n",
    "    # --- 2. 讀取 metadata 並計算分數 ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"✅ 成功讀取 metadata.json，將分析 {len(metadata)} 首歌曲。\")\n",
    "        print(f\"ℹ️  Warping Path 裁切秒數設定為: {TRIM_SECONDS}s\")\n",
    "        print(f\"ℹ️  Warping Path 降採樣步長設定為: {SUBSAMPLE_STEP}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 錯誤：找不到 metadata.json 檔案。\")\n",
    "        return\n",
    "\n",
    "    for i, song_data in enumerate(tqdm(metadata, desc=\"Analyzing Songs\")):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "\n",
    "        song_dir = os.path.join(base_dir, dir_name)\n",
    "        origin_path = os.path.join(song_dir, origin_filename)\n",
    "        if not os.path.exists(origin_path): continue\n",
    "\n",
    "        for v in versions:\n",
    "            cover_path = os.path.join(song_dir, f\"{v}.wav\")\n",
    "            if not os.path.exists(cover_path): continue\n",
    "            \n",
    "            # 【關鍵修改 #2】呼叫函式時傳入裁切秒數\n",
    "            score = calculate_nwpd(origin_path, cover_path, song_dir, \n",
    "                                   lambda_nwpd=lambda_val, \n",
    "                                   subsample_step=SUBSAMPLE_STEP,\n",
    "                                   trim_seconds=TRIM_SECONDS)\n",
    "            if score > 0:\n",
    "                results_list.append({\n",
    "                    'Version': VERSION_DISPLAY_NAMES.get(v, v), # 直接使用顯示名稱\n",
    "                    'WPS Score': score\n",
    "                })\n",
    "\n",
    "    # --- 3. 計算並打印平均分數 ---\n",
    "    df = pd.DataFrame(results_list)\n",
    "    if df.empty:\n",
    "        print(\"未能計算出任何有效分數。\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\n\\n--- 平均分數統計 ---\")\n",
    "    # 根據平均分數由高到低排序\n",
    "    sorted_means = df.groupby('Version')['WPS Score'].mean().sort_values(ascending=False)\n",
    "    print(sorted_means)\n",
    "    \n",
    "    # --- 4. 數據視覺化 ---\n",
    "    print(\"\\n🎨 正在生成分數分佈圖...\")\n",
    "    \n",
    "    # 獲取排序後的顯示名稱列表\n",
    "    order = sorted_means.index\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    sns.boxplot(data=df, x='WPS Score', y='Version', order=order, palette='viridis', ax=ax)\n",
    "\n",
    "    title = f'WPS Distribution (Trim: {TRIM_SECONDS}s, Subsample: {SUBSAMPLE_STEP})'\n",
    "    ax.set_title(title, fontsize=16, pad=20)\n",
    "    ax.set_xlabel('WPS', fontsize=12)\n",
    "    ax.set_ylabel('Version', fontsize=12)\n",
    "    ax.set_xlim(-0.05, 1.05)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 儲存圖表\n",
    "    output_image_path = f\"nwpd_score_dist_trim{int(TRIM_SECONDS)}_step{SUBSAMPLE_STEP}.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"✅ 分數分佈圖已成功儲存至: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_and_visualize_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假設您的 Synchronizer 位於此處\n",
    "from corpus import Synchronizer \n",
    "\n",
    "def calculate_nwpd_trimmed(origin_path: str, cover_path: str, song_dir: str, \n",
    "                           lambda_nwpd: float = 1.0, trim_seconds: float = 10.0) -> float:\n",
    "    \"\"\"\n",
    "    計算正規化路徑偏差 (NWPD) 分數。\n",
    "    【新版】：此版本會裁去對齊路徑的頭尾部分再進行計算。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = Synchronizer()\n",
    "        wp = s.get_wp(origin_path, cover_path, song_dir)\n",
    "        \n",
    "        t_cover = s.t1\n",
    "        t_orig = s.t2\n",
    "        \n",
    "        if t_cover is None or t_orig is None:\n",
    "            raise ValueError(\"時間戳序列未能成功在 Synchronizer 物件中生成。\")\n",
    "\n",
    "        wp_int = wp.astype(int)\n",
    "        path_t_cover = t_cover[wp_int[0]]\n",
    "        path_t_orig = t_orig[wp_int[1]]\n",
    "        \n",
    "        # --- 【關鍵修改】裁切對齊路徑的頭尾 ---\n",
    "        \n",
    "        # 1. 檢查樂曲總時長是否足夠進行裁切\n",
    "        total_duration = path_t_orig[-1]\n",
    "        \n",
    "        trimmed_path_t_cover = path_t_cover\n",
    "        trimmed_path_t_orig = path_t_orig\n",
    "\n",
    "        if total_duration > (2 * trim_seconds):\n",
    "            # 2. 找出需要保留的時間區間\n",
    "            start_time = trim_seconds\n",
    "            end_time = total_duration - trim_seconds\n",
    "            \n",
    "            # 3. 建立布林遮罩 (mask) 來過濾在這個區間內的點\n",
    "            mask = (path_t_orig >= start_time) & (path_t_orig <= end_time)\n",
    "            \n",
    "            # 應用遮罩\n",
    "            trimmed_path_t_cover = path_t_cover[mask]\n",
    "            trimmed_path_t_orig = path_t_orig[mask]\n",
    "            \n",
    "            # 如果裁切後點太少，則不進行裁切\n",
    "            if len(trimmed_path_t_orig) < 10:\n",
    "                trimmed_path_t_cover = path_t_cover\n",
    "                trimmed_path_t_orig = path_t_orig\n",
    "        \n",
    "        # --- 後續計算使用裁切後的路徑 ---\n",
    "\n",
    "        coeffs = np.polyfit(trimmed_path_t_cover, trimmed_path_t_orig, 1)\n",
    "        a, b = coeffs[0], coeffs[1]\n",
    "\n",
    "        t_orig_predicted = a * trimmed_path_t_cover + b\n",
    "        deviation = trimmed_path_t_orig - t_orig_predicted\n",
    "        sigma_dev = np.std(deviation)\n",
    "        nwpd_score = np.exp(-lambda_nwpd * sigma_dev)\n",
    "        \n",
    "        return nwpd_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"計算 NWPD 分數時發生錯誤：{e}, 檔案: {os.path.basename(cover_path)}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def analyze_and_visualize_scores_detailed():\n",
    "    \"\"\"\n",
    "    主函式：計算所有歌曲各版本的 NWPD 分數（使用裁切版），並進行分析。\n",
    "    \"\"\"\n",
    "    # --- 1. 設定 ---\n",
    "    base_dir = os.path.join(\".\", \"dataset\", \"eval\")\n",
    "    metadata_path = os.path.join(base_dir, \"metadata.json\")\n",
    "    origin_filename = \"origin.wav\"\n",
    "    versions = [\"human\", \"etude_e\", \"etude_d\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    lambda_val = 0.5\n",
    "    trim_seconds = 20.0 # 設定要裁切的秒數\n",
    "    \n",
    "    results_list = []\n",
    "\n",
    "    # --- 2. 讀取 metadata 並計算分數 ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"✅ 成功讀取 metadata.json，將分析 {len(metadata)} 首歌曲。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 錯誤：找不到 metadata.json 檔案。\")\n",
    "        return\n",
    "\n",
    "    for song_data in tqdm(metadata, desc=\"Analyzing Songs\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "\n",
    "        song_dir = os.path.join(base_dir, dir_name)\n",
    "        origin_path = os.path.join(song_dir, origin_filename)\n",
    "        if not os.path.exists(origin_path): continue\n",
    "\n",
    "        for v in versions:\n",
    "            cover_path = os.path.join(song_dir, f\"{v}.wav\")\n",
    "            if not os.path.exists(cover_path): continue\n",
    "            \n",
    "            # 【關鍵修改】呼叫新的裁切版函式\n",
    "            score = calculate_nwpd_trimmed(origin_path, cover_path, song_dir, \n",
    "                                           lambda_nwpd=lambda_val, trim_seconds=trim_seconds)\n",
    "            if score > 0:\n",
    "                results_list.append({\n",
    "                    'song': dir_name,\n",
    "                    'version': v,\n",
    "                    'nwpd_score': score\n",
    "                })\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"未能計算出任何有效分數。\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # --- 3. 輸出詳細統計與低分歌曲 ---\n",
    "    print(\"\\n\\n--- 各版本【裁切後】NWPD 分數詳細統計 ---\")\n",
    "    detailed_stats = df.groupby('version')['nwpd_score'].describe().sort_values('mean', ascending=False)\n",
    "    print(detailed_stats)\n",
    "\n",
    "    print(\"\\n--- 【裁切後】分數最低的 10 首 'human' 演奏歌曲 ---\")\n",
    "    human_scores_df = df[df['version'] == 'human']\n",
    "    lowest_human_scores = human_scores_df.sort_values(by='nwpd_score').head(10)\n",
    "    print(lowest_human_scores)\n",
    "\n",
    "    # --- 4. 數據視覺化 ---\n",
    "    print(\"\\n🎨 正在生成【裁切後】分數分佈圖 (Box Plot)...\")\n",
    "    order = detailed_stats.index\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(data=df, x='nwpd_score', y='version', order=order, palette='viridis')\n",
    "    plt.title(f'NWPD Score Distribution (Trimmed by {trim_seconds}s)', fontsize=18, pad=20)\n",
    "    plt.xlabel('NWPD Score (Higher is Better)', fontsize=14)\n",
    "    plt.ylabel('Version', fontsize=14)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    output_image_path = \"nwpd_score_boxplot_trimmed.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"✅ 分數分佈圖已成功儲存至: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 確保 pandas, seaborn, tqdm 已安裝\n",
    "    # pip install pandas seaborn tqdm\n",
    "    analyze_and_visualize_scores_detailed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm # 用於顯示進度條\n",
    "\n",
    "from evaluation import IPECalculator\n",
    "\n",
    "def analyze_dataset_for_ipe_params():\n",
    "    \"\"\"\n",
    "    遍歷資料集，計算所有人類演奏的香農熵，並輸出統計數據以決定 IPE 參數。\n",
    "    \"\"\"\n",
    "    dataset_dir = \"./dataset/synced/\"\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"錯誤：找不到資料集目錄 {dataset_dir}\")\n",
    "        return\n",
    "\n",
    "    # 初始化計算器。mu 和 sigma 在這裡不重要，但 n_gram 和 n_clusters 會影響熵的計算\n",
    "    ipe_calculator = IPECalculator(n_gram=5, n_clusters=16)\n",
    "    \n",
    "    entropy_values = []\n",
    "    \n",
    "    # 獲取所有子目錄\n",
    "    subdirectories = [d for d in os.scandir(dataset_dir) if d.is_dir()]\n",
    "    \n",
    "    print(f\"正在分析 {len(subdirectories)} 首人類演奏歌曲...\")\n",
    "    \n",
    "    # 使用 tqdm 顯示進度條\n",
    "    for entry in tqdm(subdirectories, desc=\"Analyzing songs\"):\n",
    "        json_path = os.path.join(entry.path, \"cover.json\")\n",
    "        \n",
    "        if os.path.exists(json_path):\n",
    "            # 我們只需要計算熵值\n",
    "            results = ipe_calculator.calculate_ipe(json_path)\n",
    "            if \"shannon_entropy\" in results:\n",
    "                entropy_values.append(results[\"shannon_entropy\"])\n",
    "\n",
    "    if not entropy_values:\n",
    "        print(\"未能在資料集中計算出任何熵值。\")\n",
    "        return\n",
    "        \n",
    "    # --- 統計分析 ---\n",
    "    entropy_series = pd.Series(entropy_values)\n",
    "    stats = entropy_series.describe()\n",
    "    \n",
    "    mean_entropy = stats['mean']\n",
    "    std_entropy = stats['std']\n",
    "    \n",
    "    print(\"\\n\\n--- 人類演奏資料集熵值統計分析 ---\")\n",
    "    print(stats)\n",
    "    \n",
    "    print(\"\\n--- 建議的 IPE 參數值 ---\")\n",
    "    print(f\"建議的 𝜇_Hn (mu_entropy): {mean_entropy:.4f}\")\n",
    "    print(f\"建議的 σ_c (sigma_entropy): {std_entropy:.4f}  (這是一個好的起始點，您可以根據需要調整)\")\n",
    "\n",
    "    # --- 視覺化 ---\n",
    "    print(\"\\n正在生成熵值分佈圖...\")\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(entropy_series, kde=True, bins=50)\n",
    "    plt.axvline(mean_entropy, color='r', linestyle='--', label=f'Mean: {mean_entropy:.2f}')\n",
    "    plt.axvline(mean_entropy + std_entropy, color='g', linestyle=':', label=f'+1 Std Dev: {mean_entropy + std_entropy:.2f}')\n",
    "    plt.axvline(mean_entropy - std_entropy, color='g', linestyle=':', label=f'-1 Std Dev: {mean_entropy - std_entropy:.2f}')\n",
    "    plt.title('人類演奏資料集的香農熵 (H_n) 分佈', fontsize=16)\n",
    "    plt.xlabel('Shannon Entropy', fontsize=12)\n",
    "    plt.ylabel('歌曲數量 (Count)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"ipe_entropy_distribution.png\", dpi=300)\n",
    "    print(\"✅ 熵值分佈圖已儲存為 ipe_entropy_distribution.png\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 安裝必要的函式庫\n",
    "    # pip install pandas matplotlib seaborn tqdm\n",
    "    analyze_dataset_for_ipe_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假設您的 IpeCalculator 類別儲存在 evaluation/IPE.py\n",
    "from evaluation.IPE import IPECalculator \n",
    "\n",
    "\n",
    "VERSION_DISPLAY_NAMES = {\n",
    "    \"human\": \"Human\",\n",
    "    \"etude_e\": \"Etude Extractor\",\n",
    "    \"etude_d\": \"Etude Decoder\",\n",
    "    \"picogen\": \"PiCoGen\",\n",
    "    \"amtapc\": \"AMT-APC\",\n",
    "    \"music2midi\": \"Music2MIDI\"\n",
    "}\n",
    "\n",
    "def evaluate_models_with_ipe():\n",
    "    \"\"\"\n",
    "    使用校準後的 IPE 參數，評估 eval 資料集中各個模型的表現。\n",
    "    \"\"\"\n",
    "    # --- 1. 設定 ---\n",
    "    eval_dir = \"./dataset/eval\"\n",
    "    metadata_path = os.path.join(eval_dir, \"metadata.json\")\n",
    "    versions = [\"cover\", \"picogen\", \"amtapc\", \"music2midi\", \"etude_e\", \"etude_d\"]\n",
    "    \n",
    "    # 使用您從 4751 首歌曲中分析出的黃金參數\n",
    "    EMPIRICAL_MU = 10.2402\n",
    "    EMPIRICAL_SIGMA = 0.7174\n",
    "    \n",
    "    # 初始化計算器\n",
    "    ipe_calculator = IPECalculator(\n",
    "        mu_entropy=EMPIRICAL_MU, \n",
    "        sigma_entropy=EMPIRICAL_SIGMA,\n",
    "        n_gram=5, \n",
    "        n_clusters=16 # 確保與分析時的參數一致\n",
    "    )\n",
    "\n",
    "    # --- 2. 讀取 metadata 並計算分數 ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"✅ 成功讀取 metadata.json，將分析 {len(metadata)} 首歌曲。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 錯誤：找不到 metadata.json 檔案。\")\n",
    "        return\n",
    "\n",
    "    results_list = []\n",
    "    for song_data in tqdm(metadata, desc=\"Evaluating Songs\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "\n",
    "        song_dir = os.path.join(eval_dir, dir_name)\n",
    "        \n",
    "        for version in versions:\n",
    "            midi_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "            if not os.path.exists(midi_path): continue\n",
    "\n",
    "            results = ipe_calculator.calculate_ipe(midi_path)\n",
    "            if \"error\" not in results:\n",
    "                results_list.append({\n",
    "                    \"song\": dir_name,\n",
    "                    \"version\": VERSION_DISPLAY_NAMES.get(version, version),\n",
    "                    \"ipe_score\": results[\"ipe_score\"],\n",
    "                    \"entropy\": results[\"shannon_entropy\"]\n",
    "                })\n",
    "\n",
    "    # --- 3. 使用 Pandas 進行統計分析 ---\n",
    "    if not results_list:\n",
    "        print(\"未計算出任何有效分數。\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    print(\"\\n\\n--- 各版本 IPE 分數統計摘要 ---\")\n",
    "    # 根據平均分數進行排序\n",
    "    summary = df.groupby('version')['ipe_score'].describe().sort_values('mean', ascending=False)\n",
    "    print(summary)\n",
    "    \n",
    "    print(\"\\n--- 各版本平均熵值 (與理想值 10.8956 比較) ---\")\n",
    "    mean_entropy = df.groupby('version')['entropy'].mean().sort_values(ascending=False)\n",
    "    print(mean_entropy)\n",
    "\n",
    "    # --- 4. 數據視覺化 ---\n",
    "    print(\"\\n🎨 正在生成分數分佈的箱形圖 (Box Plot)...\")\n",
    "    \n",
    "    # 根據平均分對版本進行排序，讓圖表更清晰\n",
    "    order = summary.index \n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.boxplot(data=df, x='ipe_score', y='version', order=order, palette='viridis')\n",
    "    \n",
    "    plt.title('IPE Score Distribution', fontsize=18, pad=20)\n",
    "    plt.xlabel('IPE Score', fontsize=14)\n",
    "    plt.ylabel('Version', fontsize=14)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    output_image_path = \"ipe_evaluation_results.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"✅ 評估結果圖表已成功儲存至: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_models_with_ipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假設您的 IpeCalculator 類別儲存在 evaluation/IPE.py\n",
    "from evaluation.IPE import IPECalculator \n",
    "\n",
    "def run_deep_dive_analysis():\n",
    "    \"\"\"\n",
    "    執行 IPE 指標的深度診斷分析，收集並呈現中間數據。\n",
    "    \"\"\"\n",
    "    # --- 1. 設定 ---\n",
    "    eval_dir = \"./dataset/eval\"\n",
    "    metadata_path = os.path.join(eval_dir, \"metadata.json\")\n",
    "    versions = [\"cover\", \"music2midi\", \"etude_d\", \"etude_e\", \"picogen\", \"amtapc\"]\n",
    "    \n",
    "    # 使用您校準後的參數\n",
    "    EMPIRICAL_MU = 10.2402\n",
    "    EMPIRICAL_SIGMA = 0.7174\n",
    "    \n",
    "    ipe_calculator = IPECalculator(\n",
    "        mu_entropy=EMPIRICAL_MU, \n",
    "        sigma_entropy=EMPIRICAL_SIGMA,\n",
    "        n_gram=5, \n",
    "        n_clusters=16\n",
    "    )\n",
    "    \n",
    "    # 修改 IPECalculator 的 IOI 提取方法，以回傳中間數據\n",
    "    original_get_ioi_sequence = ipe_calculator.get_ioi_sequence\n",
    "    def get_ioi_with_stats(midi_path: str):\n",
    "        # 這是一個 wrapper 函式，用於捕獲預處理前後的數據\n",
    "        try:\n",
    "            midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "            onsets = []\n",
    "            for instrument in midi_data.instruments:\n",
    "                if not instrument.is_drum:\n",
    "                    onsets.extend([note.start for note in instrument.notes])\n",
    "            if len(onsets) < 2: return None, {}\n",
    "            \n",
    "            unique_onsets = np.unique(onsets)\n",
    "            unique_onsets.sort()\n",
    "            if len(unique_onsets) < 2: return None, {}\n",
    "            \n",
    "            raw_ioi = np.diff(unique_onsets)\n",
    "            processed_ioi = ipe_calculator._process_raw_ioi(raw_ioi)\n",
    "            \n",
    "            stats = {\n",
    "                'raw_ioi_count': len(raw_ioi),\n",
    "                'processed_ioi_count': len(processed_ioi),\n",
    "                'filtered_percent': (1 - len(processed_ioi) / len(raw_ioi)) * 100 if len(raw_ioi) > 0 else 0,\n",
    "                'capped_count': np.sum(raw_ioi > ipe_calculator.max_ioi)\n",
    "            }\n",
    "            return processed_ioi, stats\n",
    "        except Exception:\n",
    "            return None, {}\n",
    "\n",
    "    ipe_calculator.get_ioi_sequence = get_ioi_with_stats # Monkey-patch a a method\n",
    "    \n",
    "    # --- 2. 收集數據 ---\n",
    "    results_list = []\n",
    "    metadata = json.load(open(metadata_path, 'r', encoding='utf-8'))\n",
    "\n",
    "    for song_data in tqdm(metadata, desc=\"Deep Dive Analysis\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "        song_dir = os.path.join(eval_dir, dir_name)\n",
    "        \n",
    "        for version in versions:\n",
    "            midi_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "            if not os.path.exists(midi_path): continue\n",
    "            \n",
    "            # 修改後的 ioi 提取方法會回傳額外統計數據\n",
    "            ioi_sequence, io_stats = ipe_calculator.get_ioi_sequence(midi_path)\n",
    "            \n",
    "            if ioi_sequence is None or ioi_sequence.size == 0: continue\n",
    "\n",
    "            # 繼續計算熵等指標\n",
    "            symbol_sequence = ipe_calculator.quantize_ioi_to_symbols(ioi_sequence)\n",
    "            if symbol_sequence.size == 0: continue\n",
    "            ngrams = ipe_calculator.get_ngrams_from_sequence(symbol_sequence, ipe_calculator.n_gram)\n",
    "            entropy = ipe_calculator.get_shannon_entropy(ngrams)\n",
    "            ipe_score = np.exp(-((entropy - ipe_calculator.mu_entropy)**2) / (2 * ipe_calculator.sigma_entropy**2))\n",
    "            \n",
    "            result_item = {\n",
    "                'song': dir_name, 'version': version, 'ipe_score': ipe_score, 'entropy': entropy\n",
    "            }\n",
    "            result_item.update(io_stats) # 將 IOI 統計數據加入結果\n",
    "            results_list.append(result_item)\n",
    "\n",
    "    # --- 3. 數據分析與呈現 ---\n",
    "    if not results_list:\n",
    "        print(\"未能收集到任何有效數據。\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # 診斷一：熵值分佈\n",
    "    print(\"\\n\\n--- 診斷一：各版本『香農熵』詳細統計 ---\")\n",
    "    entropy_stats = df.groupby('version')['entropy'].describe().sort_values('mean', ascending=False)\n",
    "    print(entropy_stats)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.kdeplot(data=df, x='entropy', hue='version', fill=True, alpha=0.5, palette='viridis')\n",
    "    plt.axvline(EMPIRICAL_MU, color='r', linestyle='--', label=f'Ideal μ: {EMPIRICAL_MU:.2f}')\n",
    "    plt.title('H Distribution', fontsize=16)\n",
    "    plt.xlabel('Shannon Entropy')\n",
    "    plt.legend()\n",
    "    plt.savefig('entropy_distribution_analysis.png', dpi=300)\n",
    "    print(\"✅ 熵值分佈圖已儲存至 entropy_distribution_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 診斷二：IOI 預處理影響\n",
    "    print(\"\\n\\n--- 診斷二：IOI 預處理影響分析 (平均值) ---\")\n",
    "    processing_stats = df.groupby('version')[['filtered_percent', 'capped_count']].mean().sort_values('filtered_percent', ascending=False)\n",
    "    print(processing_stats)\n",
    "\n",
    "    # 診斷三：K-Means 用詞習慣 (以分數差異最大的一首歌為例)\n",
    "    print(\"\\n\\n--- 診斷三：節奏符號使用頻率比較 (範例) ---\")\n",
    "    # 找到 cover 和 music2midi 分數差異最大的一首歌\n",
    "    pivot_df = df.pivot(index='song', columns='version', values='ipe_score')\n",
    "    pivot_df['diff'] = (pivot_df['music2midi'] - pivot_df['cover']).abs()\n",
    "    sample_song_dir = pivot_df.nlargest(1, 'diff').index[0]\n",
    "    print(f\"以分數差異最大的歌曲 '{sample_song_dir}' 為例進行分析:\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "    \n",
    "    for i, version in enumerate(['cover', 'music2midi']):\n",
    "        midi_path = os.path.join(eval_dir, sample_song_dir, f\"{version}.mid\")\n",
    "        ioi_sequence, _ = ipe_calculator.get_ioi_sequence(midi_path)\n",
    "        symbol_sequence = ipe_calculator.quantize_ioi_to_symbols(ioi_sequence)\n",
    "        \n",
    "        if symbol_sequence.size > 0:\n",
    "            sns.histplot(symbol_sequence, ax=axs[i], bins=ipe_calculator.n_clusters, kde=False)\n",
    "            axs[i].set_title(f\"Symbol Usage - {version} @ {sample_song_dir}\")\n",
    "            axs[i].set_xticks(range(ipe_calculator.n_clusters))\n",
    "\n",
    "    plt.suptitle(\"Freq\", fontsize=16)\n",
    "    plt.savefig('symbol_usage_analysis.png', dpi=300)\n",
    "    print(\"✅ 節奏符號使用頻率比較圖已儲存至 symbol_usage_analysis.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # monkey-patch a class a method a little bit\n",
    "    from evaluation.IPE import pretty_midi\n",
    "    run_deep_dive_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bac921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 假設您的 RGCCalculator 類別位於此處\n",
    "from evaluation import RGCCalculator\n",
    "\n",
    "def get_genre_from_dirname(dir_name: str) -> str:\n",
    "    \"\"\"\n",
    "    根據目錄名稱推斷音樂類型。\n",
    "    \"\"\"\n",
    "    dir_name_upper = dir_name.upper()\n",
    "    if \"CPOP\" in dir_name_upper:\n",
    "        return \"CPOP\"\n",
    "    elif \"JPOP\" in dir_name_upper:\n",
    "        return \"JPOP\"\n",
    "    elif \"KPOP\" in dir_name_upper:\n",
    "        return \"KPOP\"\n",
    "    elif \"WESTERN\" in dir_name_upper:\n",
    "        return \"WESTERN\"\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "def analyze_rgc_by_genre_and_overall():\n",
    "    \"\"\"\n",
    "    主函式：計算所有歌曲各版本的 RGC 分數，先按音樂類型分析，最後再進行總體分析。\n",
    "    \"\"\"\n",
    "    # --- 1. 設定 ---\n",
    "    eval_dir = \"./dataset/eval\"\n",
    "    metadata_path = os.path.join(eval_dir, \"metadata.json\")\n",
    "    versions = [\"cover\", \"picogen\", \"etude_d\", \"music2midi\", \"amtapc\", \"etude_e\"]\n",
    "    genres_to_analyze = [\"CPOP\", \"JPOP\", \"KPOP\", \"WESTERN\"]\n",
    "    \n",
    "    VERSION_DISPLAY_NAMES = {\n",
    "        \"cover\": \"Human\",\n",
    "        \"picogen\": \"PiCoGen\",\n",
    "        \"etude_d\": \"Etude Decoder\",\n",
    "        \"music2midi\": \"Music2MIDI\",\n",
    "        \"amtapc\": \"AMT-APC\",\n",
    "        \"etude_e\": \"Etude Extractor\"\n",
    "    }\n",
    "    \n",
    "    # 使用您覺得效果最好的微調參數\n",
    "    rgc_calc = RGCCalculator(\n",
    "        reasonable_bpm_range=(60, 240),\n",
    "        tau_falloff_sigma=0.03,\n",
    "        lambda_grid_fit=10.0\n",
    "    )\n",
    "\n",
    "    # --- 2. 一次性遍歷所有歌曲並計算分數 ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"✅ 成功讀取 metadata.json，將分析 {len(metadata)} 首歌曲。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 錯誤：找不到 metadata.json 檔案於 {metadata_path}\")\n",
    "        return\n",
    "        \n",
    "    results_list = []\n",
    "    for song_data in tqdm(metadata, desc=\"Calculating all RGC scores\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "        \n",
    "        genre = get_genre_from_dirname(dir_name)\n",
    "        song_dir = os.path.join(eval_dir, dir_name)\n",
    "        \n",
    "        for version in versions:\n",
    "            midi_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "            if not os.path.exists(midi_path): continue\n",
    "            \n",
    "            results = rgc_calc.calculate_rgc(midi_path)\n",
    "            if \"error\" not in results:\n",
    "                results['song'] = dir_name\n",
    "                results['version'] = version\n",
    "                results['genre'] = genre\n",
    "                results_list.append(results)\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"未能計算出任何有效分數，無法生成報告。\")\n",
    "        return\n",
    "\n",
    "    df_all = pd.DataFrame(results_list)\n",
    "    df_all['display_name'] = df_all['version'].map(VERSION_DISPLAY_NAMES)\n",
    "    \n",
    "    print(\"\\n✅ 所有歌曲分數計算完畢，開始按類型進行分析...\")\n",
    "\n",
    "    # --- 3. 按音樂類型遍歷，分別進行分析與呈現 ---\n",
    "    for genre in genres_to_analyze:\n",
    "        print(f\"\\n\\n{'='*25} 分析報告: {genre} {'='*25}\")\n",
    "        \n",
    "        df_genre = df_all[df_all['genre'] == genre].copy()\n",
    "        \n",
    "        if df_genre.empty:\n",
    "            print(f\"在資料集中找不到類型為 '{genre}' 的歌曲，已跳過。\")\n",
    "            continue\n",
    "            \n",
    "        # ... (此處省略了分項報告的 print 和繪圖邏輯，與您提供的一致) ...\n",
    "        print(f\"\\n--- {genre} 類型 RGC 分數統計 ---\")\n",
    "        summary = df_genre.groupby('display_name')['rgc_score'].describe().sort_values('mean', ascending=False)\n",
    "        print(summary)\n",
    "        \n",
    "        print(f\"\\n--- {genre} 類型『基本節拍單位 τ』統計 (秒) ---\")\n",
    "        tau_summary = df_genre.groupby('display_name')['inferred_tau'].describe()\n",
    "        print(tau_summary[['mean', 'std']])\n",
    "        \n",
    "        order = summary.index \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        sns.boxplot(data=df_genre, x='rgc_score', y='display_name', order=order, palette='plasma')\n",
    "        ax.set_title(f'RGC Score Distribution for {genre}', fontsize=18, pad=20)\n",
    "        ax.set_xlabel('RGC Score', fontsize=14)\n",
    "        ax.set_ylabel('Version', fontsize=14)\n",
    "        ax.set_xlim(-0.05, max(df_genre['rgc_score'].max() * 1.1, 0.8) if not df_genre.empty else 1.0)\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        output_image_path = f\"rgc_evaluation_{genre}.png\"\n",
    "        plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"✅ {genre} 類型評估圖表已成功儲存至: {output_image_path}\")\n",
    "\n",
    "    # --- 【關鍵修改】新增總體分析報告 ---\n",
    "    print(f\"\\n\\n{'='*25} 總體分析報告: ALL GENRES ({len(metadata)}首歌曲) {'='*25}\")\n",
    "\n",
    "    print(\"\\n--- 總體 RGC 分數統計 ---\")\n",
    "    summary_all = df_all.groupby('display_name')['rgc_score'].describe().sort_values('mean', ascending=False)\n",
    "    print(summary_all)\n",
    "\n",
    "    print(\"\\n--- 總體『基本節拍單位 τ』統計 (秒) ---\")\n",
    "    tau_summary_all = df_all.groupby('display_name')['inferred_tau'].describe()\n",
    "    print(tau_summary_all[['mean', 'std']])\n",
    "    \n",
    "    # --- 繪製總體圖表 ---\n",
    "    print(\"\\n🎨 正在生成總體分數分佈圖...\")\n",
    "    \n",
    "    order_all = summary_all.index \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    sns.boxplot(data=df_all, x='rgc_score', y='display_name', order=order_all, palette='viridis')\n",
    "    \n",
    "    ax.set_title('Overall RGC Score Distribution (All Genres)', fontsize=18, pad=20)\n",
    "    ax.set_xlabel('RGC Score', fontsize=14)\n",
    "    ax.set_ylabel('Version', fontsize=14)\n",
    "    ax.set_xlim(-0.05, max(df_all['rgc_score'].max() * 1.1, 0.8))\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    output_image_path_overall = \"rgc_evaluation_OVERALL.png\"\n",
    "    plt.savefig(output_image_path_overall, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"✅ 總體評估圖表已成功儲存至: {output_image_path_overall}\")\n",
    "    \n",
    "    print(\"\\n\\n🎉 所有類型的分析與視覺化均已完成！\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 確保您的 RGCCalculator 類別定義在 evaluation.py 中，或修改下面的 import 路徑\n",
    "    from evaluation import RGCCalculator\n",
    "    analyze_rgc_by_genre_and_overall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 假設您的 RGCCalculator 類別位於此處\n",
    "from evaluation import RGCCalculator\n",
    "from evaluation import RCCalculator\n",
    "\n",
    "\n",
    "def get_genre_from_dirname(dir_name: str) -> str:\n",
    "    \"\"\"\n",
    "    根據目錄名稱推斷音樂類型。\n",
    "    \"\"\"\n",
    "    dir_name_upper = dir_name.upper()\n",
    "    if \"CPOP\" in dir_name_upper:\n",
    "        return \"CPOP\"\n",
    "    elif \"JPOP\" in dir_name_upper:\n",
    "        return \"JPOP\"\n",
    "    elif \"KPOP\" in dir_name_upper:\n",
    "        return \"KPOP\"\n",
    "    elif \"WESTERN\" in dir_name_upper:\n",
    "        return \"WESTERN\"\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "def analyze_rgc_by_genre():\n",
    "    \"\"\"\n",
    "    主函式：計算所有歌曲各版本的 RGC 分數，然後按音樂類型分別進行分析與視覺化。\n",
    "    \"\"\"\n",
    "    # --- 1. 設定 ---\n",
    "    eval_dir = \"./dataset/eval\"\n",
    "    metadata_path = os.path.join(eval_dir, \"metadata.json\")\n",
    "    versions = [\"cover\", \"picogen\", \"etude_d\", \"etude_d_d\", \"music2midi\", \"amtapc\", \"etude_e\"]\n",
    "    genres_to_analyze = [\"CPOP\", \"JPOP\", \"KPOP\", \"WESTERN\"]\n",
    "    \n",
    "    # 使用您覺得效果最好的微調參數\n",
    "    LAMBDA_GRID_FIT = 10.0\n",
    "    \n",
    "    rc_calc = RCCalculator(\n",
    "        top_k=8,\n",
    "        lambda_grid_fit=LAMBDA_GRID_FIT\n",
    "    )\n",
    "\n",
    "    # --- 2. 一次性遍歷所有歌曲並計算分數 ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"✅ 成功讀取 metadata.json，將分析 {len(metadata)} 首歌曲。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 錯誤：找不到 metadata.json 檔案於 {metadata_path}\")\n",
    "        return\n",
    "        \n",
    "    results_list = []\n",
    "    for song_data in tqdm(metadata, desc=\"Calculating all RC scores\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "        \n",
    "        genre = get_genre_from_dirname(dir_name)\n",
    "        song_dir = os.path.join(eval_dir, dir_name)\n",
    "        \n",
    "        for version in versions:\n",
    "            midi_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "            if not os.path.exists(midi_path): continue\n",
    "            \n",
    "            results = rc_calc.calculate_rc(midi_path)\n",
    "            if \"error\" not in results:\n",
    "                results['song'] = dir_name\n",
    "                results['version'] = version\n",
    "                results['genre'] = genre # 將類型資訊加入結果\n",
    "                results_list.append(results)\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"未能計算出任何有效分數，無法生成報告。\")\n",
    "        return\n",
    "\n",
    "    # 建立包含所有結果的主 DataFrame\n",
    "    df_all = pd.DataFrame(results_list)\n",
    "    print(\"\\n✅ 所有歌曲分數計算完畢，開始按類型進行分析...\")\n",
    "\n",
    "    # --- 3. 【關鍵修改】按音樂類型遍歷，分別進行分析與呈現 ---\n",
    "    for genre in genres_to_analyze:\n",
    "        print(f\"\\n\\n{'='*25} 分析報告: {genre} {'='*25}\")\n",
    "        \n",
    "        # 篩選出當前類型的數據\n",
    "        df_genre = df_all[df_all['genre'] == genre].copy()\n",
    "        \n",
    "        if df_genre.empty:\n",
    "            print(f\"在資料集中找不到類型為 '{genre}' 的歌曲，已跳過。\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- {genre} 類型 RC 分數統計 ---\")\n",
    "        summary = df_genre.groupby('version')['rc_score'].describe().sort_values('mean', ascending=False)\n",
    "        print(summary)\n",
    "        \n",
    "        # print(f\"\\n--- {genre} 類型『基本節拍單位 τ』統計 (秒) ---\")\n",
    "        # tau_summary = df_genre.groupby('version')['inferred_tau'].describe()\n",
    "        # print(tau_summary[['mean', 'std']])\n",
    "        \n",
    "        # --- 繪製該類型的圖表 ---\n",
    "        print(f\"\\n🎨 正在生成 {genre} 類型的分數分佈圖...\")\n",
    "        \n",
    "        order = summary.index \n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, ax = plt.subplots(figsize=(12, 8)) # 為每個類型建立新的圖表\n",
    "        \n",
    "        sns.boxplot(data=df_genre, x='rc_score', y='version', order=order, palette='plasma')\n",
    "        \n",
    "        ax.set_title(f'RGC Score Distribution for {genre}', fontsize=18, pad=20)\n",
    "        ax.set_xlabel('RGC Score', fontsize=14)\n",
    "        ax.set_ylabel('Version', fontsize=14)\n",
    "        ax.set_xlim(-0.05, max(df_genre['rc_score'].max() * 1.1, 0.8))\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        output_image_path = f\"rgc_evaluation_{genre}.png\"\n",
    "        plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig) # 關閉當前的圖表，避免在下一次迴圈中重疊\n",
    "        \n",
    "        print(f\"✅ {genre} 類型評估圖表已成功儲存至: {output_image_path}\")\n",
    "\n",
    "    print(\"\\n\\n🎉 所有類型的分析與視覺化均已完成！\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_rgc_by_genre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f13da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Penalty Weights: 100%|██████████| 4/4 [01:28<00:00, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== WPD-N 權重敏感度分析報告 ====================\n",
      "\n",
      "--- 報告一：各版本在不同權重下的平均誤差值 (越低越好) ---\n",
      "penalty_weight                0.00      0.05      0.10      0.50\n",
      "version                                                         \n",
      "AMT-APC                   0.086916  1.062312  2.037707  9.840872\n",
      "Etude Decoder - Default   0.211236  0.701990  1.192744  5.118775\n",
      "Etude Decoder - Prompted  0.233434  0.829003  1.424573  6.189125\n",
      "Etude Extractor           0.118497  0.911008  1.703518  8.043605\n",
      "Human                     0.489353  0.924388  1.359422  4.839699\n",
      "Music2MIDI                0.183201  0.487121  0.791041  3.222401\n",
      "PiCoGen                   1.001302  1.386048  1.770795  4.848767\n",
      "\n",
      "\n",
      "--- 報告二：各版本在不同權重下的排名變化 (1=最好) ---\n",
      "penalty_weight            0.00  0.05  0.10  0.50\n",
      "version                                         \n",
      "AMT-APC                      1     6     7     7\n",
      "Etude Decoder - Default      4     2     2     4\n",
      "Etude Decoder - Prompted     5     3     4     5\n",
      "Etude Extractor              2     4     5     6\n",
      "Human                        6     5     3     2\n",
      "Music2MIDI                   3     1     1     1\n",
      "PiCoGen                      7     7     6     3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluation import WPDCalculator\n",
    "\n",
    "def test_density_penalty_weights():\n",
    "    \"\"\"\n",
    "    測試不同的 density_penalty_weight 值對最終模型排名的影響。\n",
    "    \"\"\"\n",
    "    # --- 1. 設定 ---\n",
    "    EVAL_DIR = \"./dataset/eval\"\n",
    "    METADATA_PATH = os.path.join(EVAL_DIR, \"metadata.json\")\n",
    "    VERSIONS = [\"cover\", \"etude_e\", \"etude_d\", \"etude_d_d\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    VERSION_DISPLAY_NAMES = {\n",
    "        \"cover\": \"Human\", \"etude_e\": \"Etude Extractor\", \"etude_d_d\": \"Etude Decoder - Default\",\n",
    "        \"etude_d\": \"Etude Decoder - Prompted\", \"picogen\": \"PiCoGen\", \"amtapc\": \"AMT-APC\", \"music2midi\": \"Music2MIDI\"\n",
    "    }\n",
    "    \n",
    "    # 【關鍵】設定一組您想要測試的權重值\n",
    "    weights_to_test = [0, 0.05, 0.1, 0.5]\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    try:\n",
    "        with open(METADATA_PATH, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 錯誤：找不到 metadata.json 檔案於 {METADATA_PATH}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. 遍歷所有權重和所有歌曲 ---\n",
    "    for weight in tqdm(weights_to_test, desc=\"Testing Penalty Weights\"):\n",
    "        # 為每個權重實例化一個新的計算器\n",
    "        wpd_calc = WPDCalculator(\n",
    "            subsample_step=1,\n",
    "            trim_seconds=10,\n",
    "            density_penalty_weight=weight\n",
    "        )\n",
    "        \n",
    "        for song_data in metadata:\n",
    "            dir_name = song_data.get(\"dir_name\")\n",
    "            if not dir_name: continue\n",
    "            \n",
    "            song_dir = os.path.join(EVAL_DIR, dir_name)\n",
    "            origin_wav_path = os.path.join(song_dir, \"origin.wav\")\n",
    "            if not os.path.exists(origin_wav_path): continue\n",
    "            \n",
    "            for version in VERSIONS:\n",
    "                # WPD-N 需要 .mid 或 .json 來計算音符密度\n",
    "                # 我們假設檔名與 .wav 對應\n",
    "                file_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "                if not os.path.exists(file_path):\n",
    "                     file_path = os.path.join(song_dir, f\"{version}.json\")\n",
    "                if not os.path.exists(file_path): continue\n",
    "\n",
    "                wav_path = os.path.join(song_dir, f\"{version}.wav\")\n",
    "                if not os.path.exists(wav_path): continue\n",
    "                \n",
    "                results = wpd_calc.calculate_wpd(origin_wav_path, file_path, song_dir)\n",
    "                if \"error\" not in results:\n",
    "                    results['version'] = VERSION_DISPLAY_NAMES.get(version, version)\n",
    "                    results['penalty_weight'] = weight\n",
    "                    all_results.append(results)\n",
    "\n",
    "    # --- 3. 使用 Pandas 進行分析與報告 ---\n",
    "    if not all_results:\n",
    "        print(\"未能計算出任何有效分數。\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*20 + \" WPD-N 權重敏感度分析報告 \" + \"=\"*20)\n",
    "\n",
    "    # 報告一：不同權重下的平均誤差值\n",
    "    print(\"\\n--- 報告一：各版本在不同權重下的平均誤差值 (越低越好) ---\")\n",
    "    pivot_scores = df.pivot_table(\n",
    "        index='version', \n",
    "        columns='penalty_weight', \n",
    "        values='wpd_score',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    print(pivot_scores)\n",
    "    \n",
    "    # 報告二：不同權重下的模型排名\n",
    "    print(\"\\n\\n--- 報告二：各版本在不同權重下的排名變化 (1=最好) ---\")\n",
    "    pivot_ranks = pivot_scores.rank(axis=0, method='min').astype(int)\n",
    "    print(pivot_ranks)\n",
    "\n",
    "test_density_penalty_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
