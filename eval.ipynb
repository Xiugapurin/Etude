{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc2ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from corpus import Synchronizer\n",
    "\n",
    "def calculate_wps(origin_path: str, cover_path: str, song_dir: str, lambda_val: float = 0.1) -> float:\n",
    "    try:\n",
    "        s = Synchronizer()\n",
    "        wp = s.get_wp(origin_path, cover_path, song_dir)\n",
    "        \n",
    "        t_cover = s.t1\n",
    "        t_orig = s.t2\n",
    "        \n",
    "        if t_cover is None or t_orig is None:\n",
    "            raise ValueError(\"æ™‚é–“æˆ³åºåˆ—æœªèƒ½æˆåŠŸåœ¨ Synchronizer ç‰©ä»¶ä¸­ç”Ÿæˆã€‚\")\n",
    "\n",
    "        wp_int = wp.astype(int)\n",
    "\n",
    "        indices_cover = wp_int[0]\n",
    "        indices_orig = wp_int[1]\n",
    "\n",
    "        time_diff_sequence = t_orig[indices_orig] - t_cover[indices_cover]\n",
    "\n",
    "        sigma_d = np.std(time_diff_sequence)\n",
    "        print(f\"wp-std: {sigma_d}\")\n",
    "        wps_score = np.exp(-lambda_val * sigma_d)\n",
    "\n",
    "        return wps_score\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"è¨ˆç®— WPS åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "        return 0.0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    target_dir = \"CPOP4\"\n",
    "    song_dir = f\"./dataset/eval/{target_dir}\"\n",
    "    original_audio = f\"{song_dir}/origin.wav\"\n",
    "    versions = [\"picogen\", \"amtapc\", \"music2midi\", \"human\"]\n",
    "\n",
    "    for v in versions:\n",
    "        cover_audio = f\"{song_dir}/{v}.wav\"\n",
    "        wps_score = calculate_wps(original_audio, cover_audio, song_dir, 0.25)\n",
    "\n",
    "        if wps_score > 0:\n",
    "            print(f\"WPS of {v}: {wps_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa20ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from corpus import Synchronizer\n",
    "\n",
    "def calculate_nwpd(origin_path: str, cover_path: str, song_dir: str, lambda_nwpd: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ­£è¦åŒ–è·¯å¾‘åå·® (Normalized Warp Path Deviation, NWPD) åˆ†æ•¸ã€‚\n",
    "    æ­¤ç‰ˆæœ¬å·²æ ¹æ“š wp çš„å¯¦éš› shape (2, L) é€²è¡Œä¿®æ­£ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = Synchronizer()\n",
    "        wp = s.get_wp(origin_path, cover_path, song_dir)\n",
    "        \n",
    "        t_cover = s.t1\n",
    "        t_orig = s.t2\n",
    "        \n",
    "        if t_cover is None or t_orig is None:\n",
    "            raise ValueError(\"æ™‚é–“æˆ³åºåˆ—æœªèƒ½æˆåŠŸåœ¨ Synchronizer ç‰©ä»¶ä¸­ç”Ÿæˆã€‚\")\n",
    "\n",
    "        wp_int = wp.astype(int)\n",
    "        path_t_cover = t_cover[wp_int[0]]\n",
    "        path_t_orig = t_orig[wp_int[1]]\n",
    "\n",
    "        coeffs = np.polyfit(path_t_cover, path_t_orig, 1)\n",
    "        a, b = coeffs[0], coeffs[1]\n",
    "\n",
    "        t_orig_predicted = a * path_t_cover + b\n",
    "        deviation = path_t_orig - t_orig_predicted\n",
    "        sigma_dev = np.std(deviation)\n",
    "        nwpd_score = np.exp(-lambda_nwpd * sigma_dev)\n",
    "        return nwpd_score\n",
    "    except Exception as e:\n",
    "        print(f\"è¨ˆç®— NWPD åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def analyze_and_visualize_scores():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½å¼ï¼šè¨ˆç®—æ‰€æœ‰æ­Œæ›²å„ç‰ˆæœ¬çš„ NWPD åˆ†æ•¸ï¼Œè¨ˆç®—å¹³å‡å€¼ï¼Œä¸¦é€²è¡Œè¦–è¦ºåŒ–ã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    base_dir = os.path.join(\".\", \"dataset\", \"eval\")\n",
    "    metadata_path = os.path.join(base_dir, \"metadata.json\")\n",
    "    origin_filename = \"origin.wav\"\n",
    "    versions = [\"human\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    lambda_val = 0.5\n",
    "    \n",
    "    # ç”¨æ–¼å„²å­˜æ‰€æœ‰åˆ†æ•¸çš„å­—å…¸\n",
    "    scores_by_version = {v: [] for v in versions}\n",
    "\n",
    "    # --- 2. è®€å– metadata ä¸¦è¨ˆç®—åˆ†æ•¸ ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        # print(f\"âœ… æˆåŠŸè®€å– metadata.jsonï¼Œå…±æ‰¾åˆ° {len(metadata)} é¦–æ­Œæ›²ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° metadata.json æª”æ¡ˆã€‚\")\n",
    "        return\n",
    "\n",
    "    for i, song_data in enumerate(metadata):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name:\n",
    "            continue\n",
    "\n",
    "        song_dir = os.path.join(base_dir, dir_name)\n",
    "        # print(f\"\\nğŸµ æ­£åœ¨è™•ç†æ­Œæ›²: {song_dir} ({i+1}/{len(metadata)})\")\n",
    "\n",
    "        origin_path = os.path.join(song_dir, origin_filename)\n",
    "        if not os.path.exists(origin_path):\n",
    "            print(f\"  â†ªï¸ å·²è·³é (æ‰¾ä¸åˆ° origin.wav)\")\n",
    "            continue\n",
    "\n",
    "        for v in versions:\n",
    "            cover_path = os.path.join(song_dir, f\"{v}.wav\")\n",
    "            if not os.path.exists(cover_path):\n",
    "                print(f\"  â†ªï¸ å·²è·³éç‰ˆæœ¬ '{v}' (æ‰¾ä¸åˆ° {v}.wav) {song_dir}\")\n",
    "                continue\n",
    "            \n",
    "            # è¨ˆç®—åˆ†æ•¸\n",
    "            score = calculate_nwpd(origin_path, cover_path, song_dir, lambda_nwpd=lambda_val)\n",
    "            if score > 0:\n",
    "                # print(f\"  ğŸ“Š ç‰ˆæœ¬ '{v}' çš„ NWPD åˆ†æ•¸: {score:.4f}\")\n",
    "                scores_by_version[v].append(score)\n",
    "\n",
    "    # --- 3. è¨ˆç®—ä¸¦æ‰“å°å¹³å‡åˆ†æ•¸ ---\n",
    "    print(\"\\n\\n--- å¹³å‡åˆ†æ•¸çµ±è¨ˆ ---\")\n",
    "    average_scores = {}\n",
    "    for version, scores in scores_by_version.items():\n",
    "        if scores:\n",
    "            avg_score = np.mean(scores)\n",
    "            average_scores[version] = avg_score\n",
    "            print(f\"ç‰ˆæœ¬ {version:<12}: å¹³å‡ NWPD åˆ†æ•¸ = {avg_score:.4f} (åŸºæ–¼ {len(scores)} å€‹æ¨£æœ¬)\")\n",
    "        else:\n",
    "            print(f\"ç‰ˆæœ¬ {version:<12}: ç„¡æœ‰æ•ˆåˆ†æ•¸å¯ä¾›è¨ˆç®—ã€‚\")\n",
    "    \n",
    "    # --- 4. æ•¸æ“šè¦–è¦ºåŒ– ---\n",
    "    print(\"\\nğŸ¨ æ­£åœ¨ç”Ÿæˆåˆ†æ•¸åˆ†ä½ˆåœ–...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # ç‚ºæ¯å€‹ç‰ˆæœ¬ç¹ªè£½ä¸€æ¢ KDE æ›²ç·š\n",
    "    for version, scores in scores_by_version.items():\n",
    "        if scores:\n",
    "            sns.kdeplot(scores, label=version, fill=True, alpha=0.5, ax=ax, lw=2.5)\n",
    "\n",
    "    ax.set_title('NWPD Score Distribution by Version', fontsize=16, pad=20)\n",
    "    ax.set_xlabel('NWPD Score', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_xlim(0, 1.05)\n",
    "    ax.legend(title='Version', fontsize=10)\n",
    "    \n",
    "    # å„²å­˜åœ–è¡¨\n",
    "    output_image_path = \"nwpd_score_distribution.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"âœ… åˆ†æ•¸åˆ†ä½ˆåœ–å·²æˆåŠŸå„²å­˜è‡³: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_and_visualize_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import IPECalculator\n",
    "\n",
    "# åˆå§‹åŒ–è¨ˆç®—å™¨ã€‚é€™è£¡çš„ mu å’Œ sigma æ‡‰è©²æ ¹æ“šæ‚¨çš„è³‡æ–™é›†é€²è¡Œè¨­å®š\n",
    "# n_clusters ä¹Ÿæ˜¯ä¸€å€‹é‡è¦çš„è¶…åƒæ•¸ï¼Œæœƒå½±éŸ¿ç¬¦è™Ÿçš„ç²’åº¦\n",
    "ipe_calc = IPECalculator(n_gram=8, n_clusters=16, mu_entropy=4.5, sigma_entropy=0.5)\n",
    "\n",
    "# å‡è¨­æ‚¨æœ‰ä¸€å€‹ MIDI æª”æ¡ˆåˆ—è¡¨\n",
    "midi_files = [\"./dataset/eval/JPOP1/cover.mid\", \"./dataset/eval/JPOP1/picogen.mid\", \"./dataset/eval/JPOP1/amtapc.mid\", \"./dataset/eval/JPOP1/music2midi.mid\"]\n",
    "\n",
    "for midi_file in midi_files:\n",
    "    print(f\"\\nAnalyzing {midi_file}...\")\n",
    "    results = ipe_calc.calculate_ipe(midi_file)\n",
    "\n",
    "    if \"error\" in results:\n",
    "        print(f\"  Error: {results['error']}\")\n",
    "    else:\n",
    "        # æ‰“å°å®Œæ•´çš„è¨ˆç®—çµæœ\n",
    "        print(\"  IPE Score: {:.4f}\".format(results['ipe_score']))\n",
    "        print(\"  Shannon Entropy: {:.4f}\".format(results['shannon_entropy']))\n",
    "        print(\"  N-gram Count: {}\".format(results['n_gram_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm # ç”¨æ–¼é¡¯ç¤ºé€²åº¦æ¢\n",
    "\n",
    "from evaluation import IPECalculator\n",
    "\n",
    "def analyze_dataset_for_ipe_params():\n",
    "    \"\"\"\n",
    "    éæ­·è³‡æ–™é›†ï¼Œè¨ˆç®—æ‰€æœ‰äººé¡æ¼”å¥çš„é¦™è¾²ç†µï¼Œä¸¦è¼¸å‡ºçµ±è¨ˆæ•¸æ“šä»¥æ±ºå®š IPE åƒæ•¸ã€‚\n",
    "    \"\"\"\n",
    "    dataset_dir = \"./dataset/synced/\"\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™é›†ç›®éŒ„ {dataset_dir}\")\n",
    "        return\n",
    "\n",
    "    # åˆå§‹åŒ–è¨ˆç®—å™¨ã€‚mu å’Œ sigma åœ¨é€™è£¡ä¸é‡è¦ï¼Œä½† n_gram å’Œ n_clusters æœƒå½±éŸ¿ç†µçš„è¨ˆç®—\n",
    "    ipe_calculator = IPECalculator(n_gram=8, n_clusters=16)\n",
    "    \n",
    "    entropy_values = []\n",
    "    \n",
    "    # ç²å–æ‰€æœ‰å­ç›®éŒ„\n",
    "    subdirectories = [d for d in os.scandir(dataset_dir) if d.is_dir()]\n",
    "    \n",
    "    print(f\"æ­£åœ¨åˆ†æ {len(subdirectories)} é¦–äººé¡æ¼”å¥æ­Œæ›²...\")\n",
    "    \n",
    "    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢\n",
    "    for entry in tqdm(subdirectories, desc=\"Analyzing songs\"):\n",
    "        json_path = os.path.join(entry.path, \"cover.json\")\n",
    "        \n",
    "        if os.path.exists(json_path):\n",
    "            # æˆ‘å€‘åªéœ€è¦è¨ˆç®—ç†µå€¼\n",
    "            results = ipe_calculator.calculate_ipe(json_path)\n",
    "            if \"shannon_entropy\" in results:\n",
    "                entropy_values.append(results[\"shannon_entropy\"])\n",
    "\n",
    "    if not entropy_values:\n",
    "        print(\"æœªèƒ½åœ¨è³‡æ–™é›†ä¸­è¨ˆç®—å‡ºä»»ä½•ç†µå€¼ã€‚\")\n",
    "        return\n",
    "        \n",
    "    # --- çµ±è¨ˆåˆ†æ ---\n",
    "    entropy_series = pd.Series(entropy_values)\n",
    "    stats = entropy_series.describe()\n",
    "    \n",
    "    mean_entropy = stats['mean']\n",
    "    std_entropy = stats['std']\n",
    "    \n",
    "    print(\"\\n\\n--- äººé¡æ¼”å¥è³‡æ–™é›†ç†µå€¼çµ±è¨ˆåˆ†æ ---\")\n",
    "    print(stats)\n",
    "    \n",
    "    print(\"\\n--- å»ºè­°çš„ IPE åƒæ•¸å€¼ ---\")\n",
    "    print(f\"å»ºè­°çš„ ğœ‡_Hn (mu_entropy): {mean_entropy:.4f}\")\n",
    "    print(f\"å»ºè­°çš„ Ïƒ_c (sigma_entropy): {std_entropy:.4f}  (é€™æ˜¯ä¸€å€‹å¥½çš„èµ·å§‹é»ï¼Œæ‚¨å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´)\")\n",
    "\n",
    "    # --- è¦–è¦ºåŒ– ---\n",
    "    print(\"\\næ­£åœ¨ç”Ÿæˆç†µå€¼åˆ†ä½ˆåœ–...\")\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(entropy_series, kde=True, bins=50)\n",
    "    plt.axvline(mean_entropy, color='r', linestyle='--', label=f'Mean: {mean_entropy:.2f}')\n",
    "    plt.axvline(mean_entropy + std_entropy, color='g', linestyle=':', label=f'+1 Std Dev: {mean_entropy + std_entropy:.2f}')\n",
    "    plt.axvline(mean_entropy - std_entropy, color='g', linestyle=':', label=f'-1 Std Dev: {mean_entropy - std_entropy:.2f}')\n",
    "    plt.title('äººé¡æ¼”å¥è³‡æ–™é›†çš„é¦™è¾²ç†µ (H_n) åˆ†ä½ˆ', fontsize=16)\n",
    "    plt.xlabel('Shannon Entropy', fontsize=12)\n",
    "    plt.ylabel('æ­Œæ›²æ•¸é‡ (Count)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"ipe_entropy_distribution.png\", dpi=300)\n",
    "    print(\"âœ… ç†µå€¼åˆ†ä½ˆåœ–å·²å„²å­˜ç‚º ipe_entropy_distribution.png\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # å®‰è£å¿…è¦çš„å‡½å¼åº«\n",
    "    # pip install pandas matplotlib seaborn tqdm\n",
    "    analyze_dataset_for_ipe_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å‡è¨­æ‚¨çš„ IpeCalculator é¡åˆ¥å„²å­˜åœ¨ evaluation/IPE.py\n",
    "from evaluation.IPE import IPECalculator \n",
    "\n",
    "def evaluate_models_with_ipe():\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ ¡æº–å¾Œçš„ IPE åƒæ•¸ï¼Œè©•ä¼° eval è³‡æ–™é›†ä¸­å„å€‹æ¨¡å‹çš„è¡¨ç¾ã€‚\n",
    "    \"\"\"\n",
    "    # --- 1. è¨­å®š ---\n",
    "    eval_dir = \"./dataset/eval\"\n",
    "    metadata_path = os.path.join(eval_dir, \"metadata.json\")\n",
    "    versions = [\"cover\", \"picogen\", \"amtapc\", \"music2midi\"]\n",
    "    \n",
    "    # ä½¿ç”¨æ‚¨å¾ 4751 é¦–æ­Œæ›²ä¸­åˆ†æå‡ºçš„é»ƒé‡‘åƒæ•¸\n",
    "    EMPIRICAL_MU = 10.8956\n",
    "    EMPIRICAL_SIGMA = 0.6923\n",
    "    \n",
    "    # åˆå§‹åŒ–è¨ˆç®—å™¨\n",
    "    ipe_calculator = IPECalculator(\n",
    "        mu_entropy=EMPIRICAL_MU, \n",
    "        sigma_entropy=EMPIRICAL_SIGMA,\n",
    "        n_gram=8, \n",
    "        n_clusters=16 # ç¢ºä¿èˆ‡åˆ†ææ™‚çš„åƒæ•¸ä¸€è‡´\n",
    "    )\n",
    "\n",
    "    # --- 2. è®€å– metadata ä¸¦è¨ˆç®—åˆ†æ•¸ ---\n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"âœ… æˆåŠŸè®€å– metadata.jsonï¼Œå°‡åˆ†æ {len(metadata)} é¦–æ­Œæ›²ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° metadata.json æª”æ¡ˆã€‚\")\n",
    "        return\n",
    "\n",
    "    results_list = []\n",
    "    for song_data in tqdm(metadata, desc=\"Evaluating Songs\"):\n",
    "        dir_name = song_data.get(\"dir_name\")\n",
    "        if not dir_name: continue\n",
    "\n",
    "        song_dir = os.path.join(eval_dir, dir_name)\n",
    "        \n",
    "        for version in versions:\n",
    "            midi_path = os.path.join(song_dir, f\"{version}.mid\")\n",
    "            if not os.path.exists(midi_path): continue\n",
    "\n",
    "            results = ipe_calculator.calculate_ipe(midi_path)\n",
    "            if \"error\" not in results:\n",
    "                results_list.append({\n",
    "                    \"song\": dir_name,\n",
    "                    \"version\": version,\n",
    "                    \"ipe_score\": results[\"ipe_score\"],\n",
    "                    \"entropy\": results[\"shannon_entropy\"]\n",
    "                })\n",
    "\n",
    "    # --- 3. ä½¿ç”¨ Pandas é€²è¡Œçµ±è¨ˆåˆ†æ ---\n",
    "    if not results_list:\n",
    "        print(\"æœªè¨ˆç®—å‡ºä»»ä½•æœ‰æ•ˆåˆ†æ•¸ã€‚\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results_list)\n",
    "    \n",
    "    print(\"\\n\\n--- å„ç‰ˆæœ¬ IPE åˆ†æ•¸çµ±è¨ˆæ‘˜è¦ ---\")\n",
    "    # æ ¹æ“šå¹³å‡åˆ†æ•¸é€²è¡Œæ’åº\n",
    "    summary = df.groupby('version')['ipe_score'].describe().sort_values('mean', ascending=False)\n",
    "    print(summary)\n",
    "    \n",
    "    print(\"\\n--- å„ç‰ˆæœ¬å¹³å‡ç†µå€¼ (èˆ‡ç†æƒ³å€¼ 10.8956 æ¯”è¼ƒ) ---\")\n",
    "    mean_entropy = df.groupby('version')['entropy'].mean().sort_values(ascending=False)\n",
    "    print(mean_entropy)\n",
    "\n",
    "    # --- 4. æ•¸æ“šè¦–è¦ºåŒ– ---\n",
    "    print(\"\\nğŸ¨ æ­£åœ¨ç”Ÿæˆåˆ†æ•¸åˆ†ä½ˆçš„ç®±å½¢åœ– (Box Plot)...\")\n",
    "    \n",
    "    # æ ¹æ“šå¹³å‡åˆ†å°ç‰ˆæœ¬é€²è¡Œæ’åºï¼Œè®“åœ–è¡¨æ›´æ¸…æ™°\n",
    "    order = summary.index \n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.boxplot(data=df, x='ipe_score', y='version', order=order, palette='viridis')\n",
    "    \n",
    "    plt.title('å„ç‰ˆæœ¬ IPE åˆ†æ•¸åˆ†ä½ˆæ¯”è¼ƒ', fontsize=18, pad=20)\n",
    "    plt.xlabel('IPE Score (è¶Šé«˜è¶Šå¥½)', fontsize=14)\n",
    "    plt.ylabel('ç‰ˆæœ¬ (Version)', fontsize=14)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    output_image_path = \"ipe_evaluation_results.png\"\n",
    "    plt.savefig(output_image_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"âœ… è©•ä¼°çµæœåœ–è¡¨å·²æˆåŠŸå„²å­˜è‡³: {output_image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_models_with_ipe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
