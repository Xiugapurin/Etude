import os
import json
from contextlib import redirect_stdout
import libtsm
import numpy as np
import soundfile as sf
import librosa
import librosa.display
from scipy.interpolate import interp1d
import tempfile
import IPython.display as ipd
from pydub import AudioSegment
import matplotlib.pyplot as plt
import scipy.interpolate
from scipy.signal import find_peaks
from synctoolbox.dtw.mrmsdtw import sync_via_mrmsdtw
from synctoolbox.dtw.utils import (
    compute_optimal_chroma_shift,
    shift_chroma_vectors,
    make_path_strictly_monotonic,
)
from synctoolbox.feature.chroma import (
    pitch_to_chroma,
    quantize_chroma,
    quantized_chroma_to_CENS,
)
from synctoolbox.feature.dlnco import pitch_onset_features_to_DLNCO
from synctoolbox.feature.pitch import audio_to_pitch_features
from synctoolbox.feature.pitch_onset import audio_to_pitch_onset_features
from synctoolbox.feature.utils import estimate_tuning

class Synchronizer:
    def __init__(self):
        self.Fs = 22050
        self.feature_rate = 50
        self.threshold_rec = 10 ** 6
        self.step_weights = np.array([1.5, 1.5, 2.0])
        self.win_len_smooth = np.array([101, 51, 21, 1])

        self.t1 = None
        self.t2 = None 


    def load_audio(self, origin_path, cover_path):
        origin_audio, _ = librosa.load(origin_path, sr=self.Fs)
        cover_audio, _ = librosa.load(cover_path, sr=self.Fs)

        return origin_audio, cover_audio

    def get_features(self, audio, tuning_offset, visualize=False):
        with redirect_stdout(open(os.devnull, "w")):
            f_pitch = audio_to_pitch_features(
                f_audio=audio,
                Fs=self.Fs,
                tuning_offset=tuning_offset,
                feature_rate=self.feature_rate,
                verbose=visualize,
            )
            f_chroma = pitch_to_chroma(f_pitch=f_pitch)
            f_chroma_quantized = quantize_chroma(f_chroma=f_chroma)

            f_pitch_onset = audio_to_pitch_onset_features(
                f_audio=audio, Fs=self.Fs, tuning_offset=tuning_offset, verbose=visualize
            )
            f_DLNCO = pitch_onset_features_to_DLNCO(
                f_peaks=f_pitch_onset,
                feature_rate=self.feature_rate,
                feature_sequence_length=f_chroma_quantized.shape[1],
                visualize=visualize,
            )
        return f_chroma_quantized, f_DLNCO


    def save_wp(self, wp, song_dir, version_key, num_frames_cover, num_frames_origin):
        json_path = os.path.join(song_dir, 'wp.json')
        all_data = {}
        if os.path.exists(json_path):
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    all_data = json.load(f)
            except json.JSONDecodeError:
                print(f"Error: {json_path} format error, creating a new file.")
        
        version_data = {
            "wp": wp.tolist(),
            "num_frames_cover": num_frames_cover,
            "num_frames_origin": num_frames_origin
        }
        all_data[version_key] = version_data

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, indent=4)


    def load_wp(self, song_dir, version_key):
        json_path = os.path.join(song_dir, 'wp.json')
        if not os.path.exists(json_path):
            return None

        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                all_data = json.load(f)
            
            version_data = all_data.get(version_key)
            if version_data and all(k in version_data for k in ["wp", "num_frames_cover", "num_frames_origin"]):
                return version_data
            return None
        except (json.JSONDecodeError, KeyError):
            print(f"Error: Fail to read or parse {json_path}.")
            return None


    def get_wp(self, origin_path, cover_path, song_dir):
        version_key = os.path.basename(cover_path).split('.')[0]

        cached_data = self.load_wp(song_dir, version_key)
        if cached_data:
            num_frames_cover = cached_data["num_frames_cover"]
            num_frames_origin = cached_data["num_frames_origin"]
            
            self.t1 = np.arange(num_frames_cover) / self.feature_rate
            self.t2 = np.arange(num_frames_origin) / self.feature_rate
            
            wp = np.array(cached_data["wp"])
            return wp.astype(int)

        origin_audio, cover_audio = self.load_audio(origin_path, cover_path)
        
        tuning_offset_1 = estimate_tuning(cover_audio, self.Fs)
        tuning_offset_2 = estimate_tuning(origin_audio, self.Fs)
        
        f_chroma_quantized_1, f_DLNCO_1 = self.get_features(cover_audio, tuning_offset_1)
        f_chroma_quantized_2, f_DLNCO_2 = self.get_features(origin_audio, tuning_offset_2)
        
        num_frames_cover = f_chroma_quantized_1.shape[1]
        num_frames_origin = f_chroma_quantized_2.shape[1]

        self.t1 = np.arange(num_frames_cover) / self.feature_rate
        self.t2 = np.arange(num_frames_origin) / self.feature_rate
        
        f_cens_1hz_1 = quantized_chroma_to_CENS(f_chroma_quantized_1, 201, 50, self.feature_rate)[0]
        f_cens_1hz_2 = quantized_chroma_to_CENS(f_chroma_quantized_2, 201, 50, self.feature_rate)[0]
        opt_chroma_shift = compute_optimal_chroma_shift(f_cens_1hz_1, f_cens_1hz_2)
        f_chroma_quantized_2 = shift_chroma_vectors(f_chroma_quantized_2, opt_chroma_shift)
        f_DLNCO_2 = shift_chroma_vectors(f_DLNCO_2, opt_chroma_shift)
        
        wp = sync_via_mrmsdtw(
            f_chroma1=f_chroma_quantized_1, f_onset1=f_DLNCO_1,
            f_chroma2=f_chroma_quantized_2, f_onset2=f_DLNCO_2,
            input_feature_rate=self.feature_rate, step_weights=self.step_weights,
            win_len_smooth=self.win_len_smooth, threshold_rec=self.threshold_rec,
            alpha=0.5,
        )
        wp = make_path_strictly_monotonic(wp)
        
        self.save_wp(wp, song_dir, version_key, num_frames_cover, num_frames_origin)

        return wp.astype(int)


    def get_wp_and_adjust(self, origin_path, cover_path, cover_json_path):
        origin_audio, cover_audio = self.load_audio(origin_path, cover_path)

        tuning_offset_1 = estimate_tuning(cover_audio, self.Fs)
        tuning_offset_2 = estimate_tuning(origin_audio, self.Fs)

        f_chroma_quantized_1, f_DLNCO_1 = self.get_features(
            cover_audio, tuning_offset_1, visualize=False
        )
        f_chroma_quantized_2, f_DLNCO_2 = self.get_features(
            origin_audio, tuning_offset_2, visualize=False
        )

        f_cens_1hz_1 = quantized_chroma_to_CENS(
            f_chroma_quantized_1, 201, 50, self.feature_rate
        )[0]
        f_cens_1hz_2 = quantized_chroma_to_CENS(
            f_chroma_quantized_2, 201, 50, self.feature_rate
        )[0]

        opt_chroma_shift = compute_optimal_chroma_shift(f_cens_1hz_1, f_cens_1hz_2)

        f_chroma_quantized_2 = shift_chroma_vectors(f_chroma_quantized_2, opt_chroma_shift)
        f_DLNCO_2 = shift_chroma_vectors(f_DLNCO_2, opt_chroma_shift)

        wp = sync_via_mrmsdtw(
            f_chroma1=f_chroma_quantized_1,
            f_onset1=f_DLNCO_1,
            f_chroma2=f_chroma_quantized_2,
            f_onset2=f_DLNCO_2,
            input_feature_rate=self.feature_rate,
            step_weights=self.step_weights,
            win_len_smooth=self.win_len_smooth,
            threshold_rec=self.threshold_rec,
            alpha=0.5,
        )
        wp = make_path_strictly_monotonic(wp)

        pitch_shift = -opt_chroma_shift % 12
        if pitch_shift > 6:
            pitch_shift -= 12

        with open(cover_json_path, 'r', encoding='utf-8') as f:
            note_data = json.load(f)

        for note in note_data:
            shifted_pitch = note['pitch'] + pitch_shift
            note['pitch'] = int(min(max(shifted_pitch, 21), 108))

        with open(cover_json_path, 'w', encoding='utf-8') as f:
            json.dump(note_data, f, indent=4)

        return wp
    
    def strong_alignment(self, origin_path, cover_path):
        origin_audio, cover_audio = self.load_audio(origin_path, cover_path)

        tuning_offset_1 = estimate_tuning(cover_audio, self.Fs)
        tuning_offset_2 = estimate_tuning(origin_audio, self.Fs)

        f_chroma_quantized_1, f_DLNCO_1 = self.get_features(
            cover_audio, tuning_offset_1, visualize=False
        )
        f_chroma_quantized_2, f_DLNCO_2 = self.get_features(
            origin_audio, tuning_offset_2, visualize=False
        )

        f_cens_1hz_1 = quantized_chroma_to_CENS(
            f_chroma_quantized_1, 201, 50, self.feature_rate
        )[0]
        f_cens_1hz_2 = quantized_chroma_to_CENS(
            f_chroma_quantized_2, 201, 50, self.feature_rate
        )[0]

        opt_chroma_shift = compute_optimal_chroma_shift(f_cens_1hz_1, f_cens_1hz_2)

        f_chroma_quantized_2 = shift_chroma_vectors(f_chroma_quantized_2, opt_chroma_shift)
        f_DLNCO_2 = shift_chroma_vectors(f_DLNCO_2, opt_chroma_shift)

        wp = sync_via_mrmsdtw(
            f_chroma1=f_chroma_quantized_1,
            f_onset1=f_DLNCO_1,
            f_chroma2=f_chroma_quantized_2,
            f_onset2=f_DLNCO_2,
            input_feature_rate=self.feature_rate,
            step_weights=self.step_weights,
            win_len_smooth=self.win_len_smooth,
            threshold_rec=self.threshold_rec,
            alpha=0.5,
        )
        wp = make_path_strictly_monotonic(wp)

        pitch_shift_for_cover_audio = -opt_chroma_shift % 12
        if pitch_shift_for_cover_audio > 6:
            pitch_shift_for_cover_audio -= 12
        cover_audio_shifted = libtsm.pitch_shift(
            cover_audio, pitch_shift_for_cover_audio * 100, order="tsm-res"
        )

        time_map = wp.T / self.feature_rate * self.Fs
        time_map[time_map[:, 0] > len(cover_audio), 0] = len(cover_audio) - 1
        time_map[time_map[:, 1] > len(origin_audio), 1] = len(origin_audio) - 1

        y_hpstsm = libtsm.hps_tsm(cover_audio_shifted, time_map)

        stereo_sonification = np.hstack((origin_audio.reshape(-1, 1), y_hpstsm))

        print("Synchronized versions", flush=True)
        ipd.display(ipd.Audio(stereo_sonification.T, rate=self.Fs, normalize=True))

        # print("Aligned Audio 1", flush=True)
        # ipd.display(ipd.Audio(y_hpstsm.T, rate=self.Fs))

    
    def weakly_align_transcription(self, origin_path, cover_path, transcription_path, time_map_path, aligned_output_path):
        """
        Perform weak alignment of origin audio and transcription, and save the result as a new JSON file and MIDI file.

        :param origin_path: Path to the original audio file (origin.wav).
        :param cover_path: Path to the cover audio file (cover.wav).
        :param transcription_path: Path to the transcription.json file (cover transcription).
        :param time_map_path: Path to the time_map.json file.
        :param aligned_output_path: Path to save the aligned transcription JSON.
        """
        # Load time_map
        if not os.path.exists(time_map_path):
            print(f"Missing time_map file: {time_map_path}")
            return

        with open(time_map_path, "r") as f:
            time_map = json.load(f)
        
        # Load transcription
        if not os.path.exists(transcription_path):
            print(f"Missing transcription file: {transcription_path}")
            return

        with open(transcription_path, "r") as f:
            transcription = json.load(f)

        origin_audio = AudioSegment.from_file(origin_path).set_channels(1)
        cover_audio = AudioSegment.from_file(cover_path).set_channels(1)

        origin_duration = origin_audio.duration_seconds
        cover_duration = cover_audio.duration_seconds
        
        # Align transcription events
        t_S0, t_P0 = time_map[0]
        t_Sn, t_Pn = time_map[-1]
        first_duration = min(t_S0, t_P0)
        last_duration = min(origin_duration - t_Sn, cover_duration - t_Pn, t_Sn - time_map[-2][0])
        time_map = [[t_S0 - first_duration, t_P0 - first_duration]] + time_map + [[t_Sn + last_duration, t_Pn + last_duration]]
        aligned_transcription = []

        for i in range(len(time_map) - 1):
            t_S1, t_P1 = time_map[i]
            t_S2, t_P2 = time_map[i + 1]

            # Select notes within the current time segment
            for note in transcription:
                if note["pitch"] > 100: continue
                t_on = note["onset"]
                t_off = note["offset"]

                if t_P1 - 0.1 <= t_on < min(t_P2, t_P1 + t_S2 - t_S1):
                    new_onset = t_on - t_P1 + t_S1
                    aligned_note = {
                        "pitch": note["pitch"],
                        "onset": new_onset,
                        "offset": new_onset + t_off - t_on,
                        "velocity": note["velocity"]
                    }
                    aligned_transcription.append(aligned_note)

        # Save aligned transcription to JSON
        with open(aligned_output_path, "w") as f:
            json.dump(aligned_transcription, f, indent=4)


    def sync_audio_files_with_downbeats(
        self, origin_audio_path, cover_audio_path, downbeats, wp
    ):
        Fs = 22050
        feature_rate = 50

        cover_audio, _ = librosa.load(cover_audio_path, sr=Fs)
        origin_audio, _ = librosa.load(origin_audio_path, sr=Fs)

        time_origin = wp[1] / feature_rate
        time_cover = wp[0] / feature_rate

        interp_func = scipy.interpolate.interp1d(time_origin, time_cover, kind="linear")

        aligned_frames = [(0, 0)]
        for db in downbeats:
            if db <= time_origin[-1]:
                corresponding_time_cover = interp_func(db)
                aligned_frames.append((int(db * Fs), int(corresponding_time_cover * Fs)))

        print("Aligned frames (in samples):", aligned_frames)

        output_audio = []
        synced_cover_audio = np.zeros_like(origin_audio)

        for i in range(len(aligned_frames)):
            start_origin, start_cover = aligned_frames[i]

            if i < len(aligned_frames) - 1:
                end_origin = aligned_frames[i + 1][0]
            else:
                end_origin = len(origin_audio) - 1

            origin_segment = origin_audio[start_origin:end_origin]

            cover_length = len(origin_segment)
            end_cover = start_cover + cover_length

            if end_cover <= len(cover_audio):
                cover_segment = cover_audio[start_cover:end_cover]
            else:
                cover_segment = np.zeros(cover_length)
                available_length = len(cover_audio) - start_cover
                if available_length > 0:
                    cover_segment[:available_length] = cover_audio[start_cover:]

            cover_segment = librosa.util.fix_length(cover_segment, size=len(origin_segment))

            synced_cover_audio[start_origin:end_origin] = cover_segment

            stereo_segment = np.vstack((origin_segment, cover_segment)).T
            output_audio.append(stereo_segment)

        stereo_audio = np.concatenate(output_audio, axis=0)
        # sf.write(output_path, stereo_audio, Fs)

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_path = temp_file.name
            sf.write(temp_path, stereo_audio, Fs)

        ipd.display(ipd.Audio(temp_path))

        return stereo_audio, synced_cover_audio
    

    def sync_audio_files_from_docs(self, path_to_warp, path_to_reference, output_path=""):
        """
        ä¸€å€‹å®Œå…¨ä¾ç…§å®˜æ–¹æ–‡ä»¶é‚è¼¯é‡å¯«çš„åŒæ­¥æ–¹æ³•ã€‚
        
        Args:
            path_to_warp (str): è¦è¢«æ‹‰ä¼¸çš„éŸ³è¨Šè·¯å¾‘ (ç­‰åŒæ–¼å®˜æ–¹æ–‡ä»¶çš„ audio_1)ã€‚
            path_to_reference (str): ä½œç‚ºåƒè€ƒåŸºæº–çš„éŸ³è¨Šè·¯å¾‘ (ç­‰åŒæ–¼å®˜æ–¹æ–‡ä»¶çš„ audio_2)ã€‚
            output_path (str): è¼¸å‡ºæª”æ¡ˆçš„è·¯å¾‘ã€‚
        """
        print("--- Running new method: sync_audio_files_from_docs ---")
        
        # 1. è¼‰å…¥éŸ³è¨Š
        audio_1, _ = librosa.load(path_to_warp, sr=self.Fs)
        audio_2, _ = librosa.load(path_to_reference, sr=self.Fs)

        # 2. ä¼°è¨ˆéŸ³æº–åç§»
        tuning_offset_1 = estimate_tuning(audio_1, self.Fs)
        tuning_offset_2 = estimate_tuning(audio_2, self.Fs)

        # 3. è¨ˆç®—ç‰¹å¾µ
        f_chroma_quantized_1, f_DLNCO_1 = self.get_features(audio_1, tuning_offset_1, visualize=False)
        f_chroma_quantized_2, f_DLNCO_2 = self.get_features(audio_2, tuning_offset_2, visualize=False)

        # 4. å°‹æ‰¾æœ€ä½³ chroma shift
        f_cens_1hz_1 = quantized_chroma_to_CENS(f_chroma_quantized_1, 201, 50, self.feature_rate)[0]
        f_cens_1hz_2 = quantized_chroma_to_CENS(f_chroma_quantized_2, 201, 50, self.feature_rate)[0]
        opt_chroma_shift = compute_optimal_chroma_shift(f_cens_1hz_1, f_cens_1hz_2)
        f_chroma_quantized_2 = shift_chroma_vectors(f_chroma_quantized_2, opt_chroma_shift)
        f_DLNCO_2 = shift_chroma_vectors(f_DLNCO_2, opt_chroma_shift)

        # 5. åŸ·è¡Œ MrMsDTW
        wp = sync_via_mrmsdtw(
            f_chroma1=f_chroma_quantized_1,
            f_onset1=f_DLNCO_1,
            f_chroma2=f_chroma_quantized_2,
            f_onset2=f_DLNCO_2,
            input_feature_rate=self.feature_rate,
            step_weights=self.step_weights,
            threshold_rec=self.threshold_rec,
            verbose=True
        )
        wp = make_path_strictly_monotonic(wp)

        # 6. éŸ³é«˜è®Šæ› (è¦è¢«æ‹‰ä¼¸çš„ audio_1)
        pitch_shift_for_audio_1 = -opt_chroma_shift % 12
        if pitch_shift_for_audio_1 > 6:
            pitch_shift_for_audio_1 -= 12
        audio_1_shifted = libtsm.pitch_shift(audio_1, pitch_shift_for_audio_1 * 100, order="tsm-res")
        
        t_warp_source = wp[0] / self.feature_rate
        t_warp_ref = wp[1] / self.feature_rate
        
        # ç‚ºäº†å»ºç«‹å…§æ’å‡½æ•¸ï¼Œæˆ‘å€‘éœ€è¦ç¢ºä¿ x è»¸ (t_warp_ref) æ˜¯åš´æ ¼éå¢çš„
        # æˆ‘å€‘éæ¿¾æ‰ t_warp_ref ä¸­æ‰€æœ‰é‡è¤‡çš„é»
        diffs = np.diff(t_warp_ref)
        mask = np.hstack([True, diffs > 0])
        
        t_warp_ref_unique = t_warp_ref[mask]
        t_warp_source_corresponding = t_warp_source[mask]

        # å»ºç«‹ä¸€å€‹å¾ã€Œåƒè€ƒæ™‚é–“ã€æ˜ å°„åˆ°ã€Œå¾…æ‹‰ä¼¸æ™‚é–“ã€çš„å…§æ’å‡½æ•¸
        # é€™å€‹å‡½æ•¸å›ç­”äº†å•é¡Œï¼šâ€œåœ¨åƒè€ƒéŸ³è¨Šçš„ t æ™‚åˆ»ï¼Œæˆ‘æ‡‰è©²å»å¾…æ‹‰ä¼¸éŸ³è¨Šçš„å“ªå€‹æ™‚åˆ»å–æ¨£ï¼Ÿâ€
        # bounds_error=False å’Œ fill_value="extrapolate" ç¢ºä¿å³ä½¿è¶…å‡ºç¯„åœä¹Ÿèƒ½å¾—åˆ°ä¸€å€‹å€¼
        interp_func = interp1d(t_warp_ref_unique, t_warp_source_corresponding, 
                               kind='linear', bounds_error=False, fill_value="extrapolate")

        # å»ºç«‹ä¸€å€‹æ•¸å­¸ä¸Šå®Œç¾çš„ã€ç·šæ€§å¢é•·çš„ç›®æ¨™æ™‚é–“è»¸ (ä»¥åƒè€ƒéŸ³è¨Šç‚ºæº–)
        # é•·åº¦èˆ‡åƒè€ƒéŸ³è¨Šçš„æ¨£æœ¬æ•¸ç›¸åŒ
        t_ref_perfect = np.arange(len(audio_2)) / self.Fs
        
        # ä½¿ç”¨å…§æ’å‡½æ•¸ï¼Œç‚ºæ¯ä¸€å€‹å®Œç¾çš„ç›®æ¨™æ™‚é–“é»ï¼Œè¨ˆç®—å‡ºå°æ‡‰çš„ä¾†æºæ™‚é–“é»
        t_source_mapped = interp_func(t_ref_perfect)
        
        # å°‡é€™å…©æ¢æ™‚é–“è»¸å †ç–Šèµ·ä¾†ï¼Œå»ºç«‹æˆ‘å€‘å…¨æ–°çš„ã€å®Œç¾çš„ time_map
        # å®ƒçš„å½¢ç‹€æ˜¯ (L, 2)ï¼Œç¬¬ä¸€æ¬„æ˜¯ä¾†æºæ™‚é–“ï¼Œç¬¬äºŒæ¬„æ˜¯ç›®æ¨™æ™‚é–“
        perfect_time_map = np.vstack([t_source_mapped, t_ref_perfect]).T
        
        # 4. åŸ·è¡Œæ™‚é–“æ‹‰ä¼¸
        # æˆ‘å€‘å‚³é audio_1 ä½œç‚ºä¾†æºï¼Œä¸¦ä½¿ç”¨é€™å€‹å®Œç¾çš„åœ°åœ–å°‡å…¶æ‹‰ä¼¸åˆ° audio_2 çš„æ™‚é–“è»¸ä¸Š
        y_warped = libtsm.pv_tsm(audio_1_shifted, perfect_time_map, Fs=self.Fs)

        print("âœ… å…¨æ–° time_map å»ºç«‹æˆåŠŸï¼Œlibtsm.pv_tsm åŸ·è¡ŒæˆåŠŸï¼")

        # 5. åˆæˆéŸ³è¨Š
        min_len = min(len(audio_2), len(y_warped))
        stereo_sonification = np.hstack((audio_2[:min_len].reshape(-1, 1), y_warped[:min_len].reshape(-1, 1)))
        
        if output_path:
            sf.write(output_path, stereo_sonification, self.Fs)
            print(f"âœ… åŒæ­¥å¾Œçš„éŸ³è¨Šå·²å„²å­˜è‡³: {output_path}")

        try:
            import IPython.display as ipd
            ipd.display(ipd.Audio(stereo_sonification.T, rate=self.Fs, normalize=True))
        except ImportError:
            pass

        return wp

    def sync_audio_files(self, origin_path, cover_path, song_dir, output_path=""):
        version_key = os.path.basename(cover_path).split('.')[0]

        # å˜—è©¦å¾å¿«å–è®€å–
        cached_data = self.load_wp(song_dir, version_key)
        if cached_data:
            wp = np.array(cached_data["wp"])
            num_frames_cover = cached_data["num_frames_cover"]
            num_frames_origin = cached_data["num_frames_origin"]
            self.t1 = np.arange(num_frames_cover) / self.feature_rate
            self.t2 = np.arange(num_frames_origin) / self.feature_rate
            print(f"âœ… å¾å¿«å–æª”æ¡ˆä¸­æˆåŠŸè®€å–ç‰ˆæœ¬ '{version_key}' çš„å®Œæ•´åŒæ­¥è³‡è¨Šã€‚")
        else:
            print(f"ğŸ’¨ å¿«å–ä¸­ç„¡ '{version_key}' çš„åŒæ­¥è³‡è¨Šï¼Œé–‹å§‹é€²è¡Œè¨ˆç®—...")
            wp = self.get_wp(origin_path, cover_path)

        origin_audio, cover_audio = self.load_audio(origin_path, cover_path)

        tuning_offset_1 = estimate_tuning(cover_audio, self.Fs)
        tuning_offset_2 = estimate_tuning(origin_audio, self.Fs)

        f_chroma_quantized_1, f_DLNCO_1 = self.get_features(
            cover_audio, tuning_offset_1, visualize=False
        )
        f_chroma_quantized_2, f_DLNCO_2 = self.get_features(
            origin_audio, tuning_offset_2, visualize=False
        )

        f_cens_1hz_1 = quantized_chroma_to_CENS(
            f_chroma_quantized_1, 201, 50, self.feature_rate
        )[0]
        f_cens_1hz_2 = quantized_chroma_to_CENS(
            f_chroma_quantized_2, 201, 50, self.feature_rate
        )[0]

        opt_chroma_shift = compute_optimal_chroma_shift(f_cens_1hz_1, f_cens_1hz_2)
        pitch_shift_for_cover_audio = -opt_chroma_shift % 12
        if pitch_shift_for_cover_audio > 6:
            pitch_shift_for_cover_audio -= 12
        
        cover_audio_shifted = libtsm.pitch_shift(
            cover_audio, pitch_shift_for_cover_audio * 100, order="tsm-res"
        )
        
        time_map = wp.T.copy() / self.feature_rate * self.Fs
        time_map[time_map[:, 0] > len(cover_audio), 0] = len(cover_audio) - 1
        time_map[time_map[:, 1] > len(origin_audio), 1] = len(origin_audio) - 1

        # 1. éæ¿¾éåš´æ ¼éå¢çš„é» (ä¿ç•™æ­¤æ­¥é©Ÿä»¥é˜²è¬ä¸€)
        diffs = np.diff(time_map, axis=0)
        mask = np.hstack([True, np.all(diffs > 1e-9, axis=1)]) # ä½¿ç”¨æ¥µå°å€¼é¿å…æµ®é»å•é¡Œ
        cleaned_time_map = time_map[mask]

        if len(cleaned_time_map) < 2:
            print("âŒ éŒ¯èª¤ï¼šæ¸…ç†å¾Œçš„å°é½Šè·¯å¾‘éçŸ­ã€‚")
            return None

        # 2. ã€é—œéµä¿®æ­£ã€‘å¼·åˆ¶ time_map çš„ç¬¬ä¸€å€‹é»å¾ (t_start, 0) é–‹å§‹
        #    libtsm è¦æ±‚ç›®æ¨™åºåˆ—(origin)çš„ç¬¬ä¸€å€‹æ™‚é–“é»å¿…é ˆç‚º 0ã€‚
        #    æˆ‘å€‘é€éæ¸›å»åç§»é‡ä¾†å¯¦ç¾é€™ä¸€é»ã€‚
        origin_time_offset = cleaned_time_map[0, 1]
        if origin_time_offset != 0:
            print(f"âš ï¸ è­¦å‘Šï¼šåµæ¸¬åˆ° origin time offset: {origin_time_offset}ã€‚æ­£åœ¨é€²è¡Œä¿®æ­£ã€‚")
            # å¾ origin æ™‚é–“åˆ—ä¸­æ¸›å»é€™å€‹åç§»ï¼Œç¢ºä¿å®ƒå¾ 0 é–‹å§‹
            cleaned_time_map[:, 1] = cleaned_time_map[:, 1] - origin_time_offset
            # ç¢ºä¿æ²’æœ‰è² å€¼ç”¢ç”Ÿ
            cleaned_time_map[cleaned_time_map[:, 1] < 0, 1] = 0

        print("\n--- Libtsm Input Pre-flight Check ---")
        print(f"Data type: {cleaned_time_map.dtype}")
        print(f"Shape: {cleaned_time_map.shape}")
        print(f"First anchor point: {cleaned_time_map[0]}")
        print(f"Is target start time zero? -> {cleaned_time_map[0, 1] == 0}")
        print(f"Is source start time non-negative? -> {cleaned_time_map[0, 0] >= 0}")
        
        final_diffs = np.diff(cleaned_time_map, axis=0)
        is_strictly_increasing = np.all(final_diffs > 0)
        print(f"Is sequence strictly increasing? -> {is_strictly_increasing}")
        if not is_strictly_increasing:
            print("  -> Found non-increasing steps AFTER final correction!")
        print("-------------------------------------\n")

        # 4. å‘¼å« libtsm
        try:
            final_map_for_libtsm = cleaned_time_map.astype(np.float32)
            y_hpstsm = libtsm.hps_tsm(cover_audio_shifted, final_map_for_libtsm)
            # ç‚ºäº†èƒ½åŸ·è¡Œï¼Œæˆ‘å€‘å…ˆå‡è¨­ y_hpstsm å­˜åœ¨
            # y_hpstsm = origin_audio
            print("âœ… libtsm.hps_tsm å‘¼å«æˆåŠŸ (æ¨¡æ“¬)ã€‚")

        except AssertionError as e:
            print(f"âŒâŒâŒ æ‰€æœ‰é˜²ç¦¦æªæ–½å‡å¤±æ•—ï¼Œå•é¡Œå¯èƒ½å‡ºåœ¨ libtsm å‡½å¼åº«æœ¬èº«: {e}")
            return None


        # --- åˆæˆèˆ‡å„²å­˜ ---
        min_len = min(len(origin_audio), len(y_hpstsm))
        stereo_sonification = np.hstack((origin_audio[:min_len].reshape(-1, 1), y_hpstsm[:min_len].reshape(-1, 1)))

        if output_path:
            sf.write(output_path, stereo_sonification, self.Fs)
            print(f"éŸ³è¨Šå·²åŒæ­¥ (ä½†æœªå¯¦éš›å¯«å…¥æª”æ¡ˆ): {output_path}")

        print("Synchronized version created.", flush=True)
        try:
            import IPython.display as ipd
            ipd.display(ipd.Audio(stereo_sonification.T, rate=self.Fs, normalize=True))
        except ImportError:
            print("IPython not found, skipping audio display.")

        return wp
