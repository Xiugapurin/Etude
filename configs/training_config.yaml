# Configuration for the main training pipeline of the EtudeDecoder model.

# --- Environment and Paths ---
environment:
  run_id: "" # A specific name for this training run
  seed: 1234

data:
  dataset_dir: "dataset/tokenized/"
  vocab_path: "dataset/vocab.json"
  data_format: "npy"
  num_workers: 4

# --- Model Architecture ---
model:
  hidden_size: 512
  num_hidden_layers: 8
  num_attention_heads: 8
  intermediate_size: 2048
  max_position_embeddings: 1024

  num_attribute_bins: 3
  attribute_emb_dim: 64

  num_classes: 3
  pad_class_id: 0
  attribute_pad_id: 0
  context_num_past_xy_pairs: 4

# --- Training Parameters ---
training:
  num_epochs: 200
  batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.98
  clip_grad_norm: 1.0

  # Learning rate scheduler
  scheduler: "cosine_with_warmup"
  warmup_epochs: 10

# --- Logging and Checkpointing ---
checkpoint:
  save_every_n_epochs: 10 # Save a permanent checkpoint every N epochs
  resume_from_checkpoint: null # Specify a run_id to resume, or null to start fresh
  output_dir: "outputs/train"
